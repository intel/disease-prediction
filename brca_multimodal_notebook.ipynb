{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ea5f1f-3f56-4d50-af00-df54361a5196",
   "metadata": {},
   "source": [
    "# Multi-Modal Disease Prediction\n",
    "## Introduction\n",
    "Many reference kits  in the bio-medical domain focus on a single-model and single-modal solution. Exclusive reliance on a single method has some limitations, such as impairing the design of robust and accurate classifiers for complex datasets. To overcome these limitations, we provide this multi-modal disease prediction reference kit.\n",
    "\n",
    "Multi-modal disease prediction is an Intel  optimized, end-to-end reference kit for fine-tuning and inference. This reference kit implements a multi-model and multi-modal solution that will help to predict diagnosis by using categorized contrast enhanced mammography data and radiologists’ notes.\n",
    " \n",
    "Check out more workflow and reference kit examples in the [Developer Catalog](https://developer.intel.com/aireferenceimplementations).\n",
    "\n",
    "## **Table of Contents**\n",
    "- [Solution Technical Overview](#solution-technical-overview)\n",
    "- [Dataset](#dataset)\n",
    "- [Validated Hardware Details](#validated-hardware-details)\n",
    "- [Software Requirements](#software-requirements)\n",
    "- [How it Works?](#how-it-works)\n",
    "    - [Architecture](#architecture)\n",
    "- [Get Started](#get-started)\n",
    "- [Download the Reference Kit Repository](#download-the-reference-kit-repository)\n",
    "- [Download and Preprocess the Datasets](#download-and-preprocess-the-datasets)\n",
    "- [Run Using Jupyter Lab](#run-using-jupyter-lab) \n",
    "- [Expected Output](#expected-output)\n",
    "- [Result Visualization](#result-visualization)\n",
    "\n",
    "<a id=\"solution-technical-overview\"></a> \n",
    "## Solution Technical Overview\n",
    "This reference kit demonstrates one possible reference implementation of a multi-model and multi-modal solution. While the vision workflow aims to train an image classifier that takes in contrast-enhanced spectral mammography (CESM) images, the natural language processing (NLP) workflow aims to train a document classifier that takes in annotation notes about a patient’s symptoms. Each pipeline creates prediction for the diagnosis of breast cancer. In the end, weighted ensemble method is used to create final prediction.\n",
    "\n",
    "The goal is to minimize an expert’s involvement in categorizing samples as normal, benign, or malignant, by developing and optimizing a decision support system that automatically categorizes the CESM with the help of radiologist notes.\n",
    "\n",
    "<a id=\"dataset\"></a> \n",
    "### DataSet\n",
    "The dataset is a collection of 2,006 high-resolution contrast-enhanced spectral mammography (CESM) images (1003 low energy images and 1003 subtracted CESM images) with annotations of 326 female patients. See Figure-1. Each patient has 8 images, 4 representing each side with two views (Top Down looking and Angled Top View) consisting of low energy and subtracted CESM images. Medical reports, written by radiologists, are provided for each case along with manual segmentation annotation for the abnormal findings in each image. As a preprocessing step, we segment the images based on the manual segmentation to get the region of interest and group annotation notes based on the subject and breast side. \n",
    "\n",
    "  ![CESM Images](assets/cesm_and_annotation.png)\n",
    "\n",
    "*Figure-2: Samples of low energy and subtracted CESM images and Medical reports, written by radiologists from the Categorized contrast enhanced mammography dataset. [(Khaled, 2022)](https://www.nature.com/articles/s41597-022-01238-0)*\n",
    "\n",
    "For more details of the dataset, visit the wikipage of the [CESM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=109379611#109379611bcab02c187174a288dbcbf95d26179e8) and read [Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research](https://www.nature.com/articles/s41597-022-01238-0).\n",
    "\n",
    "<a id=\"validated-hardware-details\"></a> \n",
    "## Validated Hardware Details\n",
    "There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running locally have the same system requirements. \n",
    "\n",
    "| Recommended Hardware         | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors| FP32, BF16 |\n",
    "| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n",
    "\n",
    "To execute the reference solution presented here, use CPU for fine tuning. \n",
    "\n",
    "<a id=\"software-requirements\"></a> \n",
    "## Software Requirements \n",
    "Linux OS (Ubuntu 22.04) is used to validate this reference solution. Make sure the following dependencies are installed.\n",
    "\n",
    "1. `sudo apt update`\n",
    "2. `sudo apt install -y build-essential gcc git libgl1-mesa-glx libglib2.0-0 python3-dev`\n",
    "3. `sudo apy install python3.9 python3-pip`, and some virtualenv like python3-venv or [conda](#1-set-up-system-software) \n",
    "4. `pip install dataset-librarian`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e233e40-cb7f-4200-997c-ef659f679972",
   "metadata": {},
   "source": [
    "<a id=\"how-it-works\"></a> \n",
    "## How It Works?\n",
    "\n",
    "<a id=\"architecture\"></a> \n",
    "### Architecture\n",
    "![Use_case_flow](assets/e2e_flow_HLS_Disease_Prediction.png)\n",
    "*Figure-1: Architecture of the reference kit* \n",
    "\n",
    "- Uses real-world CESM breast cancer datasets with “multi-modal and multi-model” approaches.\n",
    "- Two domain toolkits (Intel® Transfer Learning Toolkit and Intel® Extension for Transformers), Intel® Neural Compressor and other libs/tools and uses Hugging Face model repo and APIs for [ResNet-50](https://huggingface.co/microsoft/resnet-50) and [ClinicalBert](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) models. \n",
    "- The NLP reference Implementation component uses [HF Fine-tuning and Inference Optimization workload](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp), which is optimized for document classification. This NLP workload employs Intel® Neural Compressor and other libraries/tools and utilizes Hugging Face model repository and APIs for ClinicalBert models. The ClinicalBert model, which is pretrained with a Masked-Language-Modeling task on a large corpus of English language from MIMIC-III data, is fine-tuned with the CESM breast cancer annotation dataset to generate a new BERT model.\n",
    "- The Vision reference Implementation component uses [TLT-based vision workload](https://github.com/IntelAI/transfer-learning), which is optimized for image fine-tuning and inference. This workload utilizes Intel® Transfer Learning Tool and tfhub's ResNet-50 model to fine-tune a new convolutional neural network model with subtracted CESM image dataset. The images are preprocessed by using domain expert-defined segmented regions to reduce redundancies during training.\n",
    "- Predict diagnosis by using categorized contrast enhanced mammography images and radiologists’ notes separately and weighted ensemble method applied to results of sub-models to create the final prediction.\n",
    "\n",
    "<a id=\"get-started\"></a> \n",
    "## Get Started\n",
    "Start by defining an environment variable that will store the workspace path, this can be an existing directory or one to be created in further steps. This ENVVAR will be used for all the commands executed      using absolute paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0a021-8f3a-4944-af9f-6db5f2fd5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "WORKSPACE = f\"~/mtw/work/\" #Path can be changed by user to desired location\n",
    "os.environ['WORKSPACE']=f\"{WORKSPACE}\"\n",
    "Path(WORKSPACE).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Work dir: {}\".format(WORKSPACE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b500932-a9c4-46bb-832b-3993c2d3c88f",
   "metadata": {},
   "source": [
    "<a id=\"download-the-reference-kit-repository\"></a> \n",
    "### Download the Reference Kit Repository\n",
    "To download the repository, follow the instructions in section `Download the Reference Kit Repository` of the README.md file.\n",
    "\n",
    "The following cell changes the current working directory to path for Python, this is needed to run the cells bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ca009-a025-4d1f-97b0-b9c657ead258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(f\"{WORKSPACE}/brca_multimodal/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54cf29-6ec9-4a05-b339-50e20de98157",
   "metadata": {},
   "source": [
    "<a id=\"download-and-preprocess-the-datasets\"></a> \n",
    "### Download and Preprocess the Datasets\n",
    "Use the links below to download the image datasets. Or skip to the [Docker](#run-using-docker) section to download the dataset using a container.\n",
    "\n",
    "- [High-resolution Contrast-enhanced spectral mammography (CESM) images](https://faspex.cancerimagingarchive.net/aspera/faspex/external_deliveries/260?passcode=5335d2514638afdaf03237780dcdfec29edf4238#)\n",
    "\n",
    "Once you have downloaded and unzip the image files and placed them into the `${WORKSPACE}/brca_multimodal/data` directory, proceed by executing the following command. This command will initiate the download of segmentation and annotation data, followed by the application of segmentation and preprocessing operations.\n",
    "\n",
    "Command-line Interface:\n",
    "- -d : Directory location where the raw dataset will be saved on your system. It's also where the preprocessed dataset files will be written. If not set, a directory with the dataset name will be created.\n",
    "- --split_ratio: Split ratio of the test data, the default value is 0.1.\n",
    "\n",
    "More details of the dataset_librarian can be found [here](https://pypi.org/project/dataset-librarian/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ba2e1-fd39-463d-b332-8903f1fe2715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The first time you execute dataset_librarian you will be requested to accept the licensing agreement,\n",
    "#scroll down and accept (y) the agreement to continue. The prosses will end once the \"Preprocessing has finished\" message appears.\n",
    "import os\n",
    "import dataset_librarian as dl\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "package_path = dl.__path__[0]\n",
    "env_file_path = os.path.join(package_path, \".env\")\n",
    "USER_CONSENT = dotenv_values(env_file_path).get(\"USER_CONSENT\")\n",
    "\n",
    "command = f'python3.9 -m dataset_librarian.dataset -n brca --download --preprocess -d data/ --split_ratio 0.1; echo \"Preprocessing has finished.\"'\n",
    "\n",
    "if USER_CONSENT  == \"y\":\n",
    "    os.popen(command, 'w')\n",
    "else:\n",
    "    os.popen(command, 'w').write(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c1f8e-d4c3-4d6b-a95d-94f45f2c9b00",
   "metadata": {},
   "source": [
    "Note: See this dataset's applicable license for terms and conditions. Intel Corporation does not own the rights to this dataset and does not confer any rights to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447360c8-27d3-4d93-9594-1f3edebc70cd",
   "metadata": {},
   "source": [
    "Once preprocessing has ended, you should have the following files and directories inside the `data/` directory:\n",
    "```\n",
    "data/ \n",
    "    |\n",
    "    └──annotation/\n",
    "        |── annotation.csv\n",
    "        |── testing_data.csv\n",
    "        └── training_data.csv\n",
    "    └──CDD-CESM/\n",
    "        |── Low energy images of CDD-CESM/\n",
    "        └── Subtracted images of CDD-CESM/\n",
    "    └──Medical reports for cases/\n",
    "    └──segmented_images/\n",
    "        |── Benign/\n",
    "        |── Malignant/\n",
    "        └── Normal/\n",
    "    └──train_test_split_images\n",
    "        |── test/\n",
    "        └── train/\n",
    "    └──vision_images\n",
    "        |── Benign/\n",
    "        |── Malignant/\n",
    "        └── Normal/\n",
    "    └──Radiology manual annotations.xlsx\n",
    "    └──Radiology_hand_drawn_segmentations_v2.csv\n",
    "    └──README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bce34-5cf5-4df7-8b66-48dc56b47473",
   "metadata": {},
   "source": [
    "<a id=\"run-using-jupyter-lab\"></a> \n",
    "# Run Using Jupyter Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5c0fc-f0eb-4a16-ba25-c500ecef10c8",
   "metadata": {},
   "source": [
    "### 1. Setup Workflow\n",
    "This step involves the installation of the following components:\n",
    "\n",
    "- HF Fine-tune & Inference Optimization workflow\n",
    "- Transfer Learning based on TLT workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df07b7-8ba9-4ecd-a862-9a3904a2552f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!bash setup_workflows.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24f317-c892-4db2-99a9-79fa5e9743d5",
   "metadata": {},
   "source": [
    "### 2. Model Building Process\n",
    "\n",
    "To train the multi-model disease prediction, utilize the `breast_cancer_prediction.py` script along with the arguments outlined in the `disease_prediction_baremetal.yaml` configuration file, which has the following structure:\n",
    "\n",
    "```\n",
    "disease_prediction_baremetal.yaml\n",
    "    \n",
    "    |\n",
    "    └──overwrite_training_testing_ids\n",
    "    └──output_dir\n",
    "    └──test_size\n",
    "    └──write\n",
    "    └──nlp\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "    └──vision\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "```\n",
    "\n",
    "The `disease_prediction_baremetal.yaml` file includes the following parameters:\n",
    "\n",
    "- output_dir: specifies the location of the output model and inference results\n",
    "- write: a container parameter that is set to false for bare metal\n",
    "- nlp:\n",
    "  - finetune: runs nlp fine-tuning\n",
    "  - inference: runs nlp inference\n",
    "  - additional parameters for the HF fine-tune and inference optimization workflow (more information available [here](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp/config))\n",
    "\n",
    "- vision:\n",
    "  - finetune: runs vision fine-tuning\n",
    "  - inference: runs vision inference\n",
    "  - additional parameters for the Vision: Transfer Learning Toolkit based on TLT workflow (more information available [here](https://github.com/IntelAI/transfer-learning/tree/f2e83f1614901d44d0fdd66f983de50551691676/workflows/disease_prediction))\n",
    "\n",
    "\n",
    "\n",
    "To solely perform the fine-tuning process, set the 'finetune' parameter to True in the following command and execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aeb7f6-c6b0-44f2-b674-e4cd753183e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml --finetune True --inference False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d487fe-2931-45d8-8f07-a7cc20175a68",
   "metadata": {},
   "source": [
    "### 3. Running Inference\n",
    "After the models are trained and saved using the script from step 2, load the NLP and vision models using the inference option. This applies a weighted ensemble method to generate a final prediction. To only run inference, set the 'inference' parameter to true, the parameter 'finetune' to false and run the command provided in step 2.\n",
    "\n",
    "> Alternatively, you can combine the training and inference processes into one execution by setting both the 'finetune' and 'inference' parameters to true and running the command provided in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0f93e-615d-48bb-a663-b4ad50371e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml --finetune False --inference True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322d32e-372d-46fd-b2a7-56083969a348",
   "metadata": {},
   "source": [
    "<a id=\"expected-output\"></a> \n",
    "## Expected Output\n",
    "A successful execution of inference returns the confusion matrix of the sub-models and ensembled model, as shown in these example results: \n",
    "```\n",
    "------ Confusion Matrix for Vision model ------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign       18.0     11.000   1.000      0.486\n",
    "Malignant     5.0     32.000   0.000      0.615\n",
    "Normal       14.0      9.000  25.000      0.962\n",
    "Recall        0.6      0.865   0.521      0.652\n",
    "\n",
    "------ Confusion Matrix for NLP model ---------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     25.000      4.000     1.0      0.893\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      0.980\n",
    "Recall      0.833      0.919     1.0      0.930\n",
    "\n",
    "------ Confusion Matrix for Ensemble --------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     26.000      4.000     0.0      0.897\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      1.000\n",
    "Recall      0.867      0.919     1.0      0.939\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40238669-92d9-49db-9fe8-9045700099fe",
   "metadata": {},
   "source": [
    "<a id=\"result-visualization\"></a> \n",
    "## Result Visualization\n",
    "By utilizing the displayed widget, users can access a comprehensive overview that includes radiologists' annotation notes, corresponding subtracted CESM images, and ensemble predictions. Scroll down to see the selected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d43a36-e217-4a0d-8e0c-49a41acd44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import widget_manager\n",
    "importlib.reload(widget_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_env",
   "language": "python",
   "name": "hls_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
