{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9ea5f1f-3f56-4d50-af00-df54361a5196",
   "metadata": {},
   "source": [
    "# Multi-Modal Disease Prediction\n",
    "Many reference kits  in the bio-medical domain focus on a single-model and single-modal solution. Exclusive reliance on a single method has some limitations, such as impairing the design of robust and accurate classifiers for complex datasets. To overcome these limitations, we provide this multi-modal disease prediction reference kit.\n",
    "\n",
    "Multi-modal disease prediction is an Intel  optimized, end-to-end reference kit for fine-tuning and inference. This reference kit implements a multi-model and multi-modal solution that will help to predict diagnosis by using categorized contrast enhanced mammography data and radiologists’ notes.\n",
    " \n",
    "\n",
    "## **Table of Contents**\n",
    "- [Overview](#overview)\n",
    "- [Validated Hardware Details](#validated-hardware-details)\n",
    "- [Software Requirements](#software-requirements)\n",
    "- [How it Works?](#how-it-works)\n",
    "    - [Architecture](#architecture)\n",
    "- [Get Started](#get-started)\n",
    "- [Run Using Jupyter Lab](#run-using-jupyter-lab) \n",
    "- [Expected Output](#expected-output)\n",
    "- [Result Visualization](#result-visualization)\n",
    "\n",
    "<a id=\"overview\"></a> \n",
    "## Overview\n",
    "This reference kit demonstrates one possible reference implementation of a multi-model and multi-modal solution. While the vision workflow aims to train an image classifier that takes in contrast-enhanced spectral mammography (CESM) images, the natural language processing (NLP) workflow aims to train a document classifier that takes in annotation notes about a patient’s symptoms. Each pipeline creates prediction for the diagnosis of breast cancer. In the end, weighted ensemble method is used to create final prediction.\n",
    "\n",
    "The goal is to minimize an expert’s involvement in categorizing samples as normal, benign, or malignant, by developing and optimizing a decision support system that automatically categorizes the CESM with the help of radiologist notes.\n",
    "\n",
    "<a id=\"validated-hardware-details\"></a> \n",
    "## Validated Hardware Details\n",
    "There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running locally have the same system requirements. \n",
    "\n",
    "| Recommended Hardware         | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors| FP32, BF16 |\n",
    "| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n",
    "\n",
    "To execute the reference solution presented here, please use CPU for fine tuning. \n",
    "\n",
    "<a id=\"software-requirements\"></a> \n",
    "## Software Requirements \n",
    "Linux OS (Ubuntu 22.04) is used in this reference solution. Make sure the following dependencies are installed.\n",
    "\n",
    "1. `sudo apt update`\n",
    "2. `sudo apt install -y build-essential gcc git libgl1-mesa-glx libglib2.0-0 python3-dev`\n",
    "3. Python3.9, Pip/Conda and python3.9-venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec95594-e5bf-4e07-83bd-213f681d0fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for aagalleg: [sudo] password for aagalleg: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...Get:1 https://cli.github.com/packages stable InRelease [3917 B]\n",
      "Get:2 http://packages.microsoft.com/repos/code stable InRelease [3569 B]\n",
      "Get:3 https://download.docker.com/linux/ubuntu focal InRelease [57.7 kB]\n",
      "Get:5 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Hit:6 http://ubuntu.osuosl.org/ubuntu focal InRelease\n",
      "Get:7 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
      "Get:8 http://packages.microsoft.com/repos/code stable/main amd64 Packages [68.7 kB]\n",
      "Get:9 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages [29.7 kB]\n",
      "Get:10 https://download.docker.com/linux/ubuntu focal/stable amd64 Contents (deb) [1384 B]\n",
      "Get:11 http://packages.microsoft.com/repos/code stable/main arm64 Packages [69.0 kB]\n",
      "Get:12 http://packages.microsoft.com/repos/code stable/main armhf Packages [69.3 kB]\n",
      "Get:13 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [445 kB]\n",
      "Get:14 http://ubuntu.osuosl.org/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:4 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease\n",
      "\n",
      "Reading state information...\n",
      "Get:15 https://releases.jfrog.io/artifactory/jfrog-debs xenial InRelease [4665 B]\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
      "build-essential is already the newest version (12.8ubuntu1.1).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  docutils-common libjs-sphinxdoc libjs-underscore python-babel-localedata\n",
      "  python3-alabaster python3-babel python3-docutils python3-feedparser\n",
      "  python3-imagesize python3-numpydoc python3-ply python3-roman\n",
      "  python3-setproctitle python3-sigmavirus24-urltemplate python3-sphinx\n",
      "  sphinx-common\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "The following additional packages will be installed:\n",
      "  libglib2.0-bin\n",
      "Suggested packages:\n",
      "  git-daemon-run | git-daemon-sysvinit git-doc git-el git-email git-gui gitk\n",
      "  gitweb git-cvs git-mediawiki git-svn\n",
      "The following NEW packages will be installed:\n",
      "  libgl1-mesa-glx\n",
      "The following packages will be upgraded:\n",
      "  git libglib2.0-0 libglib2.0-bin\n",
      "Get:16 http://ubuntu.osuosl.org/ubuntu focal-security InRelease [114 kB]\n",
      "Get:17 https://releases.jfrog.io/artifactory/jfrog-debs xenial/contrib amd64 Packages [16.1 kB]\n",
      "Get:18 http://ubuntu.osuosl.org/ubuntu focal-backports InRelease [108 kB]\n",
      "3 upgraded, 1 newly installed, 0 to remove and 484 not upgraded.\n",
      "Need to get 5899 kB of archives.\n",
      "After this operation, 77.8 kB of additional disk space will be used.\n",
      "Get:1 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 libglib2.0-bin amd64 2.64.6-1~ubuntu20.04.4 [72.6 kB]\n",
      "Get:19 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 Packages [2571 kB]\n",
      "Get:2 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 libglib2.0-0 amd64 2.64.6-1~ubuntu20.04.4 [1287 kB]\n",
      "Get:20 http://ubuntu.osuosl.org/ubuntu focal-updates/main Translation-en [434 kB]\n",
      "Get:21 http://ubuntu.osuosl.org/ubuntu focal-updates amd64 Contents (deb) [188 MB]\n",
      "Ign:3 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 git amd64 1:2.25.1-1ubuntu3.10\n",
      "Get:4 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 libgl1-mesa-glx amd64 21.2.6-0ubuntu0.1~20.04.2 [5536 B]\n",
      "Err:3 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 git amd64 1:2.25.1-1ubuntu3.10\n",
      "  404  Not Found [IP: 10.7.211.16 911]\n",
      "Fetched 1365 kB in 1s (1515 kB/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E: Failed to fetch http://ubuntu.osuosl.org/ubuntu/pool/main/g/git/git_2.25.1-1ubuntu3.10_amd64.deb  404  Not Found [IP: 10.7.211.16 911]\n",
      "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:22 http://ubuntu.osuosl.org/ubuntu focal-updates/main amd64 c-n-f Metadata [16.6 kB]\n",
      "Get:23 http://ubuntu.osuosl.org/ubuntu focal-updates/universe amd64 Packages [1063 kB]\n",
      "Get:24 http://ubuntu.osuosl.org/ubuntu focal-updates/universe Translation-en [252 kB]\n",
      "Get:25 http://ubuntu.osuosl.org/ubuntu focal-updates/universe amd64 c-n-f Metadata [24.3 kB]\n",
      "Get:26 http://ubuntu.osuosl.org/ubuntu focal-updates/restricted amd64 Packages [1879 kB]\n",
      "Get:27 http://ubuntu.osuosl.org/ubuntu focal-updates/restricted Translation-en [264 kB]\n",
      "Get:28 http://ubuntu.osuosl.org/ubuntu focal-updates/restricted amd64 c-n-f Metadata [636 B]\n",
      "Get:29 http://ubuntu.osuosl.org/ubuntu focal-updates/multiverse amd64 Packages [25.2 kB]\n",
      "Get:30 http://ubuntu.osuosl.org/ubuntu focal-updates/multiverse amd64 c-n-f Metadata [612 B]\n",
      "Get:31 http://ubuntu.osuosl.org/ubuntu focal-security/main amd64 Packages [2191 kB]\n",
      "Get:32 http://ubuntu.osuosl.org/ubuntu focal-security/main Translation-en [353 kB]\n",
      "Get:33 http://ubuntu.osuosl.org/ubuntu focal-security amd64 Contents (deb) [175 MB]\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = 'sudo -S apt-get update'\n",
    "os.popen(command, 'w').write(password+'\\n')\n",
    "command = 'sudo -S apt-get install -y build-essential gcc git libgl1-mesa-glx libglib2.0-0 python3-dev'\n",
    "os.popen(command, 'w').write(password+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e233e40-cb7f-4200-997c-ef659f679972",
   "metadata": {},
   "source": [
    "<a id=\"how-it-works\"></a> \n",
    "## How It Works?\n",
    "\n",
    "<a id=\"architecture\"></a> \n",
    "### Architecture\n",
    "![Use_case_flow](assets/e2e_flow_HLS_Disease_Prediction.png)\n",
    "*Figure-1: Architecture of the reference kit* \n",
    "\n",
    "- Uses real-world CESM breast cancer datasets with “multi-modal and multi-model” approaches.\n",
    "- Two domain toolkits (Intel® Transfer Learning Toolkit and Intel® Extension for Transformers), Intel® Neural Compressor and other libs/tools and uses Hugging Face model repo and APIs for [ResNet-50](https://huggingface.co/microsoft/resnet-50) and [ClinicalBert](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) models. \n",
    "- The NLP reference Implementation component uses [HF Fine-tuning and Inference Optimization workload](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp), which is optimized for document classification. This NLP workload employs Intel® Neural Compressor and other libraries/tools and utilizes Hugging Face model repository and APIs for ClinicalBert models. The ClinicalBert model, which is pretrained with a Masked-Language-Modeling task on a large corpus of English language from MIMIC-III data, is fine-tuned with the CESM breast cancer annotation dataset to generate a new BERT model.\n",
    "- The Vision reference Implementation component uses [TLT-based vision workload](https://github.com/IntelAI/transfer-learning), which is optimized for image fine-tuning and inference. This workload utilizes Intel® Transfer Learning Tool and tfhub's ResNet-50 model to fine-tune a new convolutional neural network model with subtracted CESM image dataset. The images are preprocessed by using domain expert-defined segmented regions to reduce redundancies during training.\n",
    "- Predict diagnosis by using categorized contrast enhanced mammography images and radiologists’ notes separately and weighted ensemble method applied to results of sub-models to create the final prediction.\n",
    "\n",
    "<a id=\"get-started\"></a> \n",
    "## Get Started\n",
    "\n",
    "### Download the Reference Kit Repository\n",
    "If previously not done, from terminal create a working directory for the reference kit and clone the [Breast Cancer Prediction Reference Kit](https://github.com/intel/disease-prediction) repository into your working directory.\n",
    "```\n",
    "git clone https://github.com/intel/disease-prediction.git brca_multimodal\n",
    "cd brca_multimodal\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d9a697e-db85-4995-8c45-52dd94d9ce46",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "The dataset is a collection of 2,006 high-resolution contrast-enhanced spectral mammography (CESM) images (1003 low energy images and 1003 subtracted CESM images) with annotations of 326 female patients. See Figure-1. Each patient has 8 images, 4 representing each side with two views (Top Down looking and Angled Top View) consisting of low energy and subtracted CESM images. Medical reports, written by radiologists, are provided for each case along with manual segmentation annotation for the abnormal findings in each image. As a preprocessing step, we segment the images based on the manual segmentation to get the region of interest and group annotation notes based on the subject and breast side. \n",
    "\n",
    "  ![CESM Images](assets/cesm_and_annotation.png)\n",
    "\n",
    "*Figure-2: Samples of low energy and subtracted CESM images and Medical reports, written by radiologists from the Categorized contrast enhanced mammography dataset. [(Khaled, 2022)](https://www.nature.com/articles/s41597-022-01238-0)*\n",
    "\n",
    "For more details of the dataset, visit the wikipage of the [CESM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=109379611#109379611bcab02c187174a288dbcbf95d26179e8) and read [Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research](https://www.nature.com/articles/s41597-022-01238-0).\n",
    "\n",
    "#### Setting Up the Data\n",
    "Use the links below to download the image datasets.\n",
    "\n",
    "- [High-resolution Contrast-enhanced spectral mammography (CESM) images](https://faspex.cancerimagingarchive.net/aspera/faspex/external_deliveries/260?passcode=5335d2514638afdaf03237780dcdfec29edf4238#)\n",
    "\n",
    "Copy all the downloaded files into the *data* directory.\n",
    "\n",
    "**Note:** See this dataset's applicable license for terms and conditions. Intel Corporation does not own the rights to this dataset and does not confer any rights to it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "859bce34-5cf5-4df7-8b66-48dc56b47473",
   "metadata": {},
   "source": [
    "<a id=\"run-using-jupyter-lab\"></a> \n",
    "# Run Using Jupyter Lab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab0e33d4-8990-446b-8617-61049d5a9003",
   "metadata": {},
   "source": [
    "### 1. Create Conda Environment \n",
    "\n",
    "If not previously created, users are encouraged to use python virtual environments for consistent package management\n",
    "\n",
    "Using virtualenv:\n",
    "\n",
    "```\n",
    "python3.9 -m venv hls_env\n",
    "source hls_env/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4d5ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipykernel\n",
      "  Using cached ipykernel-6.23.1-py3-none-any.whl (152 kB)\n",
      "Collecting pyzmq>=20\n",
      "  Using cached pyzmq-25.0.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting traitlets>=5.4.0\n",
      "  Using cached traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
      "Collecting ipython>=7.23.1\n",
      "  Using cached ipython-8.13.2-py3-none-any.whl (797 kB)\n",
      "Collecting nest-asyncio\n",
      "  Using cached nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting jupyter-client>=6.1.12\n",
      "  Using cached jupyter_client-8.2.0-py3-none-any.whl (103 kB)\n",
      "Collecting jupyter-core!=5.0.*,>=4.12\n",
      "  Using cached jupyter_core-5.3.0-py3-none-any.whl (93 kB)\n",
      "Collecting matplotlib-inline>=0.1\n",
      "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting psutil\n",
      "  Using cached psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "Collecting debugpy>=1.6.5\n",
      "  Using cached debugpy-1.6.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting tornado>=6.1\n",
      "  Using cached tornado-6.3.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "Collecting comm>=0.1.1\n",
      "  Using cached comm-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Collecting stack-data\n",
      "  Using cached stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
      "Collecting decorator\n",
      "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting jedi>=0.16\n",
      "  Using cached jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30\n",
      "  Using cached prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n",
      "Collecting pygments>=2.4.0\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Collecting pickleshare\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.6.1-py3-none-any.whl (31 kB)\n",
      "Collecting pexpect>4.3\n",
      "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting backcall\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting importlib-metadata>=4.8.3\n",
      "  Using cached importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
      "Collecting platformdirs>=2.5\n",
      "  Using cached platformdirs-3.5.1-py3-none-any.whl (15 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting parso<0.9.0,>=0.8.0\n",
      "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth\n",
      "  Using cached wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pure-eval\n",
      "  Using cached pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting asttokens>=2.1.0\n",
      "  Using cached asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting executing>=1.2.0\n",
      "  Using cached executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wcwidth, pure-eval, ptyprocess, pickleshare, executing, backcall, zipp, typing-extensions, traitlets, tornado, six, pyzmq, pygments, psutil, prompt-toolkit, platformdirs, pexpect, parso, packaging, nest-asyncio, decorator, debugpy, python-dateutil, matplotlib-inline, jupyter-core, jedi, importlib-metadata, comm, asttokens, stack-data, jupyter-client, ipython, ipykernel\n",
      "Successfully installed asttokens-2.2.1 backcall-0.2.0 comm-0.1.3 debugpy-1.6.7 decorator-5.1.1 executing-1.2.0 importlib-metadata-6.6.0 ipykernel-6.23.1 ipython-8.13.2 jedi-0.18.2 jupyter-client-8.2.0 jupyter-core-5.3.0 matplotlib-inline-0.1.6 nest-asyncio-1.5.6 packaging-23.1 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.5.1 prompt-toolkit-3.0.38 psutil-5.9.5 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.15.1 python-dateutil-2.8.2 pyzmq-25.0.2 six-1.16.0 stack-data-0.6.2 tornado-6.3.2 traitlets-5.9.0 typing-extensions-4.6.1 wcwidth-0.2.6 zipp-3.15.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalled kernelspec hls_env in /ec/pdx/disks/mlp_lab_home_pool_02/aagalleg/.local/share/jupyter/kernels/hls_env\n"
     ]
    }
   ],
   "source": [
    "!python3.9 -m venv hls_env\n",
    "!hls_env/bin/python3 -m pip install ipykernel\n",
    "!hls_env/bin/python3 -m ipykernel install --user --name hls_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097f09ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978c9625",
   "metadata": {},
   "source": [
    "**NOTE:** Restart kernel and wait some seconds, and change kernel to hls_env. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b445eb4",
   "metadata": {},
   "source": [
    "Or conda: If you don't already have conda installed, see the [Conda Linux installation instructions](https://docs.conda.io/projects/conda/en/stable/user-guide/install/linux.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e68bd-2dc4-40ce-856d-d821f366dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name hls_env python=3.9 ipykernel -y -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe3d63-5f51-4f1f-aabf-8c3e05abbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify hls_env environment is active\n",
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f59e7e-233f-4fab-ba31-4f140ba30a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6d53da4-e598-4850-bef2-59acd36022e4",
   "metadata": {},
   "source": [
    "**NOTE:** Restart kernel and wait some seconds, and change kernel to conda env:hls_env. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e5c0fc-f0eb-4a16-ba25-c500ecef10c8",
   "metadata": {},
   "source": [
    "### 2. Install Required Workflows and Preprocess the Data\n",
    "\n",
    "> Make sure [High-resolution Contrast-enhanced spectral mammography (CESM) images](https://faspex.cancerimagingarchive.net/aspera/faspex/external_deliveries/260?passcode=5335d2514638afdaf03237780dcdfec29edf4238#) are downloaded and copied into the *data* directory.\n",
    "\n",
    "This step involves the installation of the following components:\n",
    "\n",
    "- HF Fine-tune & Inference Optimization workflow\n",
    "- Transfer Learning based on TLT workflow\n",
    "- Model Zoo for Intel®\n",
    "\n",
    "Upon successful installation, the data is preprocessed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92fccae6-105b-42cd-a123-12b23cddfc09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Collecting jupyterlab\n",
      "  Using cached jupyterlab-4.0.0-py3-none-any.whl (9.2 MB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Using cached jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (8.13.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Using cached widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (6.23.1)\n",
      "Requirement already satisfied: packaging in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (23.1)\n",
      "Collecting jinja2>=3.0.3\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting async-lru>=1.0.0\n",
      "  Using cached async_lru-2.0.2-py3-none-any.whl (5.7 kB)\n",
      "Collecting jupyter-lsp>=2.0.0\n",
      "  Using cached jupyter_lsp-2.1.0-py3-none-any.whl (64 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0\n",
      "  Using cached jupyter_server-2.5.0-py3-none-any.whl (366 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (6.3.2)\n",
      "Collecting jupyterlab-server<3,>=2.19.0\n",
      "  Using cached jupyterlab_server-2.22.1-py3-none-any.whl (57 kB)\n",
      "Collecting notebook-shim>=0.2\n",
      "  Using cached notebook_shim-0.2.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: jupyter-core in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (5.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (6.6.0)\n",
      "Collecting tomli\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./hls_env/lib/python3.9/site-packages (from async-lru>=1.0.0->jupyterlab) (4.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./hls_env/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyterlab) (3.15.0)\n",
      "Requirement already satisfied: pyzmq>=20 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
      "Requirement already satisfied: nest-asyncio in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: decorator in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./hls_env/lib/python3.9/site-packages (from jupyter-core->jupyterlab) (3.5.1)\n",
      "Collecting websocket-client\n",
      "  Using cached websocket_client-1.5.2-py3-none-any.whl (56 kB)\n",
      "Collecting nbconvert>=6.4.4\n",
      "  Using cached nbconvert-7.4.0-py3-none-any.whl (285 kB)\n",
      "Collecting anyio>=3.1.0\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "Collecting argon2-cffi\n",
      "  Using cached argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting jupyter-server-terminals\n",
      "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.17.0-py3-none-any.whl (60 kB)\n",
      "Collecting jupyter-events>=0.4.0\n",
      "  Using cached jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
      "Collecting nbformat>=5.3.0\n",
      "  Using cached nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
      "Collecting send2trash\n",
      "  Using cached Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Using cached terminado-0.17.1-py3-none-any.whl (17 kB)\n",
      "Collecting babel>=2.10\n",
      "  Using cached Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "Collecting json5>=0.9.0\n",
      "  Using cached json5-0.9.14-py2.py3-none-any.whl (19 kB)\n",
      "Collecting jsonschema>=4.17.3\n",
      "  Using cached jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Collecting requests>=2.28\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./hls_env/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Collecting attrs>=17.4.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Collecting python-json-logger>=2.0.4\n",
      "  Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
      "Collecting rfc3986-validator>=0.1.1\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting pyyaml>=5.3\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Collecting rfc3339-validator\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting tinycss2\n",
      "  Using cached tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Using cached nbclient-0.8.0-py3-none-any.whl (73 kB)\n",
      "Collecting mistune<3,>=2.0.3\n",
      "  Using cached mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-6.0.0-py3-none-any.whl (162 kB)\n",
      "Collecting fastjsonschema\n",
      "  Using cached fastjsonschema-2.17.1-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./hls_env/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Requirement already satisfied: pure-eval in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: six in ./hls_env/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Collecting webcolors>=1.11\n",
      "  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\n",
      "Collecting isoduration\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Collecting uri-template\n",
      "  Using cached uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting jsonpointer>1.13\n",
      "  Using cached jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting fqdn\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting cffi>=1.0.1\n",
      "  Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting arrow>=0.15.0\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Using legacy 'setup.py install' for docx2txt, since package 'wheel' is not installed.\n",
      "Installing collected packages: webencodings, mistune, json5, fastjsonschema, docx2txt, widgetsnbextension, websocket-client, webcolors, urllib3, uri-template, tomli, tinycss2, terminado, soupsieve, sniffio, send2trash, rfc3986-validator, rfc3339-validator, pyyaml, python-json-logger, python-dotenv, pyrsistent, pycparser, prometheus-client, pandocfilters, MarkupSafe, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, idna, fqdn, et-xmlfile, defusedxml, charset-normalizer, certifi, bleach, babel, attrs, async-lru, requests, openpyxl, jupyter-server-terminals, jsonschema, jinja2, cffi, beautifulsoup4, arrow, anyio, nbformat, isoduration, argon2-cffi-bindings, nbclient, argon2-cffi, nbconvert, jupyter-events, ipywidgets, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab\n",
      "  Running setup.py install for docx2txt ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed MarkupSafe-2.1.2 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 async-lru-2.0.2 attrs-23.1.0 babel-2.12.1 beautifulsoup4-4.12.2 bleach-6.0.0 certifi-2023.5.7 cffi-1.15.1 charset-normalizer-3.1.0 defusedxml-0.7.1 docx2txt-0.8 et-xmlfile-1.1.0 fastjsonschema-2.17.1 fqdn-1.5.1 idna-3.4 ipywidgets-8.0.6 isoduration-20.11.0 jinja2-3.1.2 json5-0.9.14 jsonpointer-2.3 jsonschema-4.17.3 jupyter-events-0.6.3 jupyter-lsp-2.1.0 jupyter-server-2.5.0 jupyter-server-terminals-0.4.4 jupyterlab-4.0.0 jupyterlab-pygments-0.2.2 jupyterlab-server-2.22.1 jupyterlab-widgets-3.0.7 mistune-2.0.5 nbclient-0.8.0 nbconvert-7.4.0 nbformat-5.8.0 notebook-shim-0.2.3 openpyxl-3.1.2 pandocfilters-1.5.0 prometheus-client-0.17.0 pycparser-2.21 pyrsistent-0.19.3 python-dotenv-1.0.0 python-json-logger-2.0.7 pyyaml-6.0 requests-2.31.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 send2trash-1.8.2 sniffio-1.3.0 soupsieve-2.4.1 terminado-0.17.1 tinycss2-1.2.1 tomli-2.0.1 uri-template-1.2.0 urllib3-2.0.2 webcolors-1.13 webencodings-0.5.1 websocket-client-1.5.2 widgetsnbextension-4.0.7\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting intel-tensorflow==2.11.0\n",
      "  Using cached intel_tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (237.5 MB)\n",
      "Collecting neural-compressor==2.0\n",
      "  Using cached neural_compressor-2.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting numpy==1.22.0\n",
      "  Using cached numpy-1.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting pillow\n",
      "  Using cached Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: pyyaml in ./hls_env/lib/python3.9/site-packages (from -r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 5)) (6.0)\n",
      "Collecting tensorflow-hub==0.12.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: packaging in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (4.6.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.9-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (58.1.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: requests in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.31.0)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting prettytable\n",
      "  Using cached prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Collecting deprecated\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.6-cp39-cp39-linux_x86_64.whl\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (5.9.5)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (3.1.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from pandas->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prettytable->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (0.2.6)\n",
      "Collecting matplotlib>=2.1.0\n",
      "  Using cached matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./hls_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (6.6.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./hls_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./hls_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (3.15.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, pytz, py-cpuinfo, libclang, flatbuffers, wrapt, wheel, werkzeug, urllib3, tzdata, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, prettytable, pillow, oauthlib, numpy, kiwisolver, keras, joblib, importlib-resources, grpcio, google-pasta, gast, fonttools, cycler, contextlib2, cachetools, absl-py, tensorflow-hub, scipy, schema, rsa, pyasn1-modules, pandas, opt-einsum, opencv-python, markdown, h5py, deprecated, contourpy, astunparse, scikit-learn, requests-oauthlib, matplotlib, google-auth, pycocotools, google-auth-oauthlib, tensorboard, neural-compressor, intel-tensorflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 contextlib2-21.6.0 contourpy-1.0.7 cycler-0.11.0 deprecated-1.2.13 flatbuffers-23.5.9 fonttools-4.39.4 gast-0.4.0 google-auth-2.18.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 importlib-resources-5.12.0 intel-tensorflow-2.11.0 joblib-1.2.0 keras-2.11.0 kiwisolver-1.4.4 libclang-16.0.0 markdown-3.4.3 matplotlib-3.7.1 neural-compressor-2.0 numpy-1.22.0 oauthlib-3.2.2 opencv-python-4.7.0.72 opt-einsum-3.3.0 pandas-2.0.1 pillow-9.5.0 prettytable-3.7.0 protobuf-3.19.6 py-cpuinfo-9.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycocotools-2.0.6 pyparsing-3.0.9 pytz-2023.3 requests-oauthlib-1.3.1 rsa-4.9 schema-0.7.5 scikit-learn-1.2.2 scipy-1.10.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 threadpoolctl-3.1.0 tzdata-2023.3 urllib3-1.26.16 werkzeug-2.3.4 wheel-0.40.0 wrapt-1.15.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting transformers==4.26.0\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting datasets==2.11.0\n",
      "  Using cached datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting neural-compressor==2.1\n",
      "  Using cached neural_compressor-2.1-py3-none-any.whl (1.2 MB)\n",
      "Collecting torch==1.13.1\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp39-cp39-linux_x86_64.whl (199.1 MB)\n",
      "Collecting intel_extension_for_pytorch==1.13.100\n",
      "  Using cached intel_extension_for_pytorch-1.13.100-cp39-cp39-manylinux2014_x86_64.whl (38.0 MB)\n",
      "Collecting intel-extension-for-transformers==1.0.0\n",
      "  Using cached intel_extension_for_transformers-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.8 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (23.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: requests in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (2.31.0)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: pandas in ./hls_env/lib/python3.9/site-packages (from datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2.0.1)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pycocotools in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: schema in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: deprecated in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.13)\n",
      "Requirement already satisfied: py-cpuinfo in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: Pillow in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: opencv-python in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (4.7.0.72)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: prettytable in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.7.0)\n",
      "Requirement already satisfied: scikit-learn in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions in ./hls_env/lib/python3.9/site-packages (from torch==1.13.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 5)) (4.6.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./hls_env/lib/python3.9/site-packages (from aiohttp->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./hls_env/lib/python3.9/site-packages (from aiohttp->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./hls_env/lib/python3.9/site-packages (from deprecated->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prettytable->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in ./hls_env/lib/python3.9/site-packages (from pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in ./hls_env/lib/python3.9/site-packages (from schema->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (21.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (5.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (4.39.4)\n",
      "Requirement already satisfied: six>=1.5 in ./hls_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./hls_env/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.15.0)\n",
      "Installing collected packages: tokenizers, xxhash, tqdm, torch, regex, pyarrow, multidict, intel_extension_for_pytorch, fsspec, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, neural-compressor, intel-extension-for-transformers, datasets\n",
      "  Attempting uninstall: neural-compressor\n",
      "    Found existing installation: neural-compressor 2.0\n",
      "    Uninstalling neural-compressor-2.0:\n",
      "      Successfully uninstalled neural-compressor-2.0\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 intel-extension-for-transformers-1.0.0 intel_extension_for_pytorch-1.13.100 multidict-6.0.4 multiprocess-0.70.14 neural-compressor-2.1 pyarrow-12.0.0 regex-2023.5.5 responses-0.18.0 tokenizers-0.13.3 torch-1.13.1+cpu tqdm-4.65.0 transformers-4.26.0 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m## Purpose of the Model Zoo\n",
      "\n",
      "  - Demonstrate the AI workloads and deep learning models Intel has optimized and validated to run on Intel hardware\n",
      "  - Show how to efficiently execute, train, and deploy Intel-optimized models\n",
      "  - Make it easy to get started running Intel-optimized models on Intel hardware in the cloud or on bare metal\n",
      "\n",
      "***DISCLAIMER: These scripts are not intended for benchmarking Intel platforms.\n",
      "For any performance and/or benchmarking information on specific Intel platforms, visit https://www.intel.ai/blog.***\n",
      "\n",
      "Intel is committed to the respect of human rights and avoiding complicity in human rights abuses, a policy reflected at https://www.intel.com/content/www/us/en/policy/policy-human-rights.html. Accordingly, by accessing the Intel material on this platform you agree that you will not use the material in a product or application that causes or contributes to a violation of an internationally recognized human right.\n",
      "\n",
      "## License\n",
      "The Model Zoo for Intel® Architecture is licensed under https://github.com/IntelAI/models/blob/master/LICENSE.\n",
      "\n",
      "## Datasets\n",
      "To the extent that any public datasets are referenced by Intel or accessed using tools or code on this site those datasets are provided by the third party indicated as the data source. Intel does not create the data, or datasets, and does not warrant their accuracy or quality. By accessing the public dataset(s) you agree to the terms associated with those datasets and that your use complies with the applicable license.\n",
      "\n",
      "Please check the list of datasets used in Model Zoo for Intel® Architecture in https://github.com/IntelAI/models/datasets directory.\n",
      "\n",
      "Intel expressly disclaims the accuracy, adequacy, or completeness of any public datasets, and is not liable for any errors, omissions, or defects in the data, or for any reliance on the data. Intel is not liable for any liability or damages relating to your use of public datasets.\n",
      "Terms and conditions agreed\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: pandas in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: docx2txt in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.8)\n",
      "Requirement already satisfied: openpyxl in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: pillow in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (9.5.0)\n",
      "Collecting scikit-learn-intelex\n",
      "  Using cached scikit_learn_intelex-2023.1.1-py39-none-manylinux1_x86_64.whl (115 kB)\n",
      "Requirement already satisfied: python-dotenv in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from openpyxl->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from scikit-learn-intelex->-r requirements.txt (line 6)) (1.2.2)\n",
      "Collecting daal4py==2023.1.1\n",
      "  Using cached daal4py-2023.1.1-py39-none-manylinux1_x86_64.whl (13.9 MB)\n",
      "Collecting daal==2023.1.1\n",
      "  Using cached daal-2023.1.1-py2.py3-none-manylinux1_x86_64.whl (69.2 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Using cached tbb-2021.9.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied: six>=1.5 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from scikit-learn>=0.22->scikit-learn-intelex->-r requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from scikit-learn>=0.22->scikit-learn-intelex->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages (from scikit-learn>=0.22->scikit-learn-intelex->-r requirements.txt (line 6)) (3.1.0)\n",
      "Installing collected packages: tbb, daal, daal4py, scikit-learn-intelex\n",
      "Successfully installed daal-2023.1.1 daal4py-2023.1.1 scikit-learn-intelex-2023.1.1 tbb-2021.9.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "File already exists in /localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/Medical reports for cases .zip.\n",
      "Please delete it and try again!.\n",
      "\n",
      "\n",
      "File already exists in /localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/Radiology manual annotations.xlsx.\n",
      "Please delete it and try again!.\n",
      "\n",
      "\n",
      "File already exists in /localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/Radiology_hand_drawn_segmentations_v2.csv.\n",
      "Please delete it and try again!.\n",
      "\n",
      "----- Starting the data preprocessing -----\n",
      "----- file is saved here : /localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/annotation/annotation.csv\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Create folders for classified images\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/\n",
      "----- Classifying the images for data preprocessing -----\n",
      "Classify Low energy images\n",
      "Classify Subtracted energy images\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/intel-models/datasets/dataset_api/../../../data/segmented_images/Normal/\n",
      "Creating segmented images for \"Normal\" cases .........\n",
      "Creating segmented images for \"malignant\" cases ..........\n",
      "Creating segmented images for \"Benign\" cases ........\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Execute this cell only if you agree with the terms and conditions from intel-models/datasets/dataset_api/terms_and_conditions.txt\n",
    "import IPython\n",
    "!source hls_env/bin/activate; echo \"y\" | bash setup_workflows.sh\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca24f317-c892-4db2-99a9-79fa5e9743d5",
   "metadata": {},
   "source": [
    "### 3. Model Building Process\n",
    "\n",
    "To train the multi-model disease prediction, utilize the 'breast_cancer_prediction.py' script along with the arguments outlined in the 'disease_prediction_baremetal.yaml' configuration file, which has the following structure:\n",
    "\n",
    "```\n",
    "disease_prediction_baremetal.yaml\n",
    "    \n",
    "    |\n",
    "    └──overwrite_training_testing_ids\n",
    "    └──output_dir\n",
    "    └──test_size\n",
    "    └──write\n",
    "    └──nlp\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "    └──vision\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "```\n",
    "\n",
    "The 'disease_prediction_baremetal.yaml' file includes the following parameters:\n",
    "\n",
    "- overwrite_training_testing_ids: uses previously created train and test data sets\n",
    "- output_dir: specifies the location of the output model and inference results\n",
    "- test_size: sets the percentage of the test data set\n",
    "- write: a container parameter that is set to false for bare metal\n",
    "- nlp:\n",
    "  - finetune: runs nlp fine-tuning\n",
    "  - inference: runs nlp inference\n",
    "  - additional parameters for the HF fine-tune and inference optimization workflow (more information available [here](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp/config))\n",
    "\n",
    "- vision:\n",
    "  - finetune: runs vision fine-tuning\n",
    "  - inference: runs vision inference\n",
    "  - additional parameters for the Vision: Transfer Learning Toolkit based on TLT workflow (more information available [here](https://github.com/IntelAI/transfer-learning/tree/f2e83f1614901d44d0fdd66f983de50551691676/workflows/disease_prediction))\n",
    "\n",
    "\n",
    "To solely perform the fine-tuning process, set the 'finetune' parameter to true in the 'disease_prediction.yaml' file and execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aeb7f6-c6b0-44f2-b674-e4cd753183e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training or Testing data does not exist. Creating them.\n",
      "2023-05-24 16:51:34.020497: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Downloading and preparing dataset csv/default to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-acc301aba94b40b1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 250.24it/s]\n",
      "Dataset csv downloaded and prepared to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-acc301aba94b40b1/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-6d523bb0bf750fdd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2052.01it/s]\n",
      "Dataset csv downloaded and prepared to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-6d523bb0bf750fdd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2023-05-24 16:51:49 [INFO] ***** Running training *****\n",
      "2023-05-24 16:51:49 [INFO]   Num examples = 507\n",
      "2023-05-24 16:51:49 [INFO]   Num Epochs = 8\n",
      "2023-05-24 16:51:49 [INFO]   Instantaneous batch size per device = 100\n",
      "2023-05-24 16:51:49 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "2023-05-24 16:51:49 [INFO]   Gradient Accumulation steps = 1\n",
      "2023-05-24 16:51:49 [INFO]   Total optimization steps = 48\n",
      "  0%|                                                    | 0/48 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████████████| 48/48 [01:40<00:00,  1.69s/it]2023-05-24 16:53:29 [INFO] \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 100.1879, 'train_samples_per_second': 40.484, 'train_steps_per_second': 0.479, 'train_loss': 0.3787533442179362, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 48/48 [01:40<00:00,  2.09s/it]\n",
      "2023-05-24 16:53:29 [INFO] Saving model checkpoint to /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp\n",
      "Configuration saved in /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/config.json\n",
      "Model weights saved in /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/pytorch_model.bin\n",
      "tokenizer config file saved in /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/tokenizer_config.json\n",
      "Special tokens file saved in /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/special_tokens_map.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 57\n",
      "  Batch size = 100\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 298.63it/s]\n",
      "\n",
      "*********** TEST_METRICS ***********\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 507\n",
      "  Batch size = 100\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:02<00:00,  2.53it/s]\n",
      "2023-05-24 16:53:34.592858: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Found cached dataset csv (/nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-6d523bb0bf750fdd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-6d523bb0bf750fdd/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-16055449e68bb171.arrow\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/nlp.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 57\n",
      "  Batch size = 100\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1210.13it/s]\n",
      "\n",
      "*********** TEST_METRICS ***********\n",
      "Accuracy: 0.9649122807017544\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 57\n",
      "  Batch size = 100\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2371.00it/s]\n",
      "['/localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/src/../../../', '/localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/src', '/localdisk/agallegos/miniconda3/lib/python39.zip', '/localdisk/agallegos/miniconda3/lib/python3.9', '/localdisk/agallegos/miniconda3/lib/python3.9/lib-dynload', '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages']\n",
      "2023-05-24 16:53:44.470006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "Model Loading time (s):  0.015523195266723633\n",
      "Found 1410 files belonging to 3 classes.\n",
      "2023-05-24 16:53:46.122007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 16:53:46.129985: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Class names: ['Benign', 'Malignant', 'Normal']\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:From /localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Enabling auto_mixed_precision_mkl\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "Checkpoint directory: /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/vision/resnet_v1_50_checkpoints\n",
      "Epoch 1/5\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-05-24 16:53:51.363381: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:388] NOTE: auto_mixed_precision_mkl is deprecated. Please use auto_mixed_precision_onednn_bfloat16 instead\n",
      "2023-05-24 16:53:51.432213: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 204/1018 nodes to bfloat16 precision using 2 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "     35/Unknown - 25s 612ms/step - loss: 0.8086 - acc: 0.59382023-05-24 16:54:14.276506: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "     36/Unknown - 26s 613ms/step - loss: 0.4881 - acc: 0.75002023-05-24 16:54:15.514411: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 195/809 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "36/36 [==============================] - 32s 804ms/step - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.7629 - val_acc: 0.6705 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 24s 669ms/step - loss: 0.4729 - acc: 0.7812 - val_loss: 0.7172 - val_acc: 0.6860 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 25s 694ms/step - loss: 0.3658 - acc: 0.8438 - val_loss: 0.7063 - val_acc: 0.6977 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 24s 679ms/step - loss: 0.2531 - acc: 0.9062 - val_loss: 0.7438 - val_acc: 0.7016 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "36/36 [==============================] - 24s 669ms/step - loss: 0.2033 - acc: 0.9375 - val_loss: 0.8053 - val_acc: 0.7287 - lr: 0.0010\n",
      "\n",
      "Total Vision Finetuning time (s):  131.84439659118652\n",
      "9/9 [==============================] - 4s 490ms/step - loss: 0.8053 - acc: 0.7287\n",
      "loss: 0.8053409457206726\n",
      "acc: 0.7286821603775024\n",
      "dict_metrics: {'e2e_training_time': 131.84439659118652, 'loss': 0.8053409457206726, 'acc': 0.7286821603775024}\n",
      "Finished Fine-tuning the vision model...\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Saved model directory: /localdisk/aagalleg/demo_bc_test/disease-prediction/src/../output/vision/resnet_v1_50/1\n",
      "Done finetuning the vision model ............\n",
      "Found 1410 files belonging to 3 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "\n",
      " Vision Model Loading time:  4.409891843795776\n",
      "Found 1410 files belonging to 3 classes.\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-05-24 16:56:12.487393: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 197/815 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "45/45 [==============================] - 25s 534ms/step - loss: 0.8124 - acc: 0.7518\n",
      "loss: 0.8124439716339111\n",
      "acc: 0.7517730593681335\n",
      "Infering data in folder:  Benign\n",
      "2023-05-24 16:56:36.904928: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 194/752 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "Infering data in folder:  Malignant\n",
      "Infering data in folder:  Normal\n",
      "Vision inference time:  159.76991868019104\n",
      "['/localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/src/../../../', '/localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/src', '/localdisk/agallegos/miniconda3/lib/python39.zip', '/localdisk/agallegos/miniconda3/lib/python3.9', '/localdisk/agallegos/miniconda3/lib/python3.9/lib-dynload', '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages']\n",
      "2023-05-24 16:59:17.785557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Found 1410 files belonging to 3 classes.\n",
      "2023-05-24 16:59:19.431195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 16:59:19.435745: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "\n",
      " Vision Model Loading time:  4.368522644042969\n",
      "Found 169 files belonging to 3 classes.\n",
      "/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "6/6 [==============================] - 3s 371ms/step - loss: 1.1515 - acc: 0.5799\n",
      "loss: 1.1514618396759033\n",
      "acc: 0.5798816680908203\n",
      "Infering data in folder:  Normal\n",
      "Infering data in folder:  Malignant\n",
      "Infering data in folder:  Benign\n",
      "Vision inference time:  17.272226572036743\n",
      "        Confusion Matrix for vision_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal       13.0   4.000      8.000   0.520\n",
      "Benign        0.0  11.000      8.000   0.579\n",
      "Malignant     0.0   0.000     13.000   1.000\n",
      "Precision     1.0   0.733      0.448   0.649\n",
      "\n",
      "        Confusion Matrix for nlp_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal     25.000     0.0      0.000   1.000\n",
      "Benign      1.000    17.0      1.000   0.895\n",
      "Malignant   0.000     0.0     13.000   1.000\n",
      "Precision   0.962     1.0      0.929   0.965\n",
      "\n",
      "        Confusion Matrix for ensemble_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal     25.000     0.0      0.000   1.000\n",
      "Benign      1.000    16.0      2.000   0.842\n",
      "Malignant   0.000     0.0     13.000   1.000\n",
      "Precision   0.962     1.0      0.867   0.947\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!source hls_env/bin/activate; python src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d487fe-2931-45d8-8f07-a7cc20175a68",
   "metadata": {},
   "source": [
    "### 4. Running Inference\n",
    "After the models are trained and saved using the script from step 4, load the NLP and vision models using the inference option. This applies a weighted ensemble method to generate a final prediction. To only run inference, set the 'inference' parameter to true in the 'disease_prediction.yaml' file and run the command provided in step 4.\n",
    "\n",
    "> Alternatively, you can combine the training and inference processes into one execution by setting both the 'finetune' and 'inference' parameters to true in the 'disease_prediction.yaml' file and running the command provided in step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322d32e-372d-46fd-b2a7-56083969a348",
   "metadata": {},
   "source": [
    "<a id=\"expected-output\"></a> \n",
    "## Expected Output\n",
    "A successful execution of inference returns the confusion matrix of the sub-models and ensembled model, as shown in these example results: \n",
    "```\n",
    "------ Confusion Matrix for Vision model ------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign       18.0     11.000   1.000      0.486\n",
    "Malignant     5.0     32.000   0.000      0.615\n",
    "Normal       14.0      9.000  25.000      0.962\n",
    "Recall        0.6      0.865   0.521      0.652\n",
    "\n",
    "------ Confusion Matrix for NLP model ---------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     25.000      4.000     1.0      0.893\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      0.980\n",
    "Recall      0.833      0.919     1.0      0.930\n",
    "\n",
    "------ Confusion Matrix for Ensemble --------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     26.000      4.000     0.0      0.897\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      1.000\n",
    "Recall      0.867      0.919     1.0      0.939\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40238669-92d9-49db-9fe8-9045700099fe",
   "metadata": {},
   "source": [
    "<a id=\"result-visualization\"></a> \n",
    "## Result Visualization\n",
    "The following cell displays the study id, images from mammogram and obtained ensemble results from training. Scroll down to see the selected results.\r\n",
    "In order for the widget to work properly, you must shut down jupyter lab, activate the \"hls_env\" environment and run jupyter lab again from the activate environment; once it’s done return to the notebook and run the following cell\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d43a36-e217-4a0d-8e0c-49a41acd44a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0eaee058374ce2b179b0c33969e7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Select(description='Patient ID:', layout=Layout(grid_area='widget001'), options=('Sel…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4bf32abaf5476685c0779f02f36a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Select(description='Patient ID:', layout=Layout(grid_area='widget001'), options=('Sel…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'widget_manager' from '/localdisk/aagalleg/demo_bc_test/disease-prediction/widget_manager.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import widget_manager\n",
    "importlib.reload(widget_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_env",
   "language": "python",
   "name": "hls_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
