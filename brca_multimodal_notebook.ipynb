{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ea5f1f-3f56-4d50-af00-df54361a5196",
   "metadata": {},
   "source": [
    "# Multi-Modal Disease Prediction\n",
    "Many reference kits  in the bio-medical domain focus on a single-model and single-modal solution. Exclusive reliance on a single method has some limitations, such as impairing the design of robust and accurate classifiers for complex datasets. To overcome these limitations, we provide this multi-modal disease prediction reference kit.\n",
    "\n",
    "Multi-modal disease prediction is an Intel  optimized, end-to-end reference kit for fine-tuning and inference. This reference kit implements a multi-model and multi-modal solution that will help to predict diagnosis by using categorized contrast enhanced mammography data and radiologists’ notes.\n",
    " \n",
    "\n",
    "## **Table of Contents**\n",
    "- [Overview](#overview)\n",
    "- [Validated Hardware Details](#validated-hardware-details)\n",
    "- [Software Requirements](#software-requirements)\n",
    "- [How it Works?](#how-it-works)\n",
    "    - [Architecture](#architecture)\n",
    "- [Get Started](#get-started)\n",
    "- [Run Using Jupyter Lab](#run-using-jupyter-lab) \n",
    "- [Expected Output](#expected-output)\n",
    "- [Result Visualization](#result-visualization)\n",
    "\n",
    "<a id=\"overview\"></a> \n",
    "## Overview\n",
    "This reference kit demonstrates one possible reference implementation of a multi-model and multi-modal solution. While the vision workflow aims to train an image classifier that takes in contrast-enhanced spectral mammography (CESM) images, the natural language processing (NLP) workflow aims to train a document classifier that takes in annotation notes about a patient’s symptoms. Each pipeline creates a prediction for the diagnosis of breast cancer. In the end, weighted ensemble method is used to create a final prediction.\n",
    "\n",
    "The goal is to minimize an expert’s involvement in categorizing samples as normal, benign, or malignant, by developing and optimizing a decision support system that automatically categorizes the CESM with the help of radiologist notes.\n",
    "\n",
    "<a id=\"validated-hardware-details\"></a> \n",
    "## Validated Hardware Details\n",
    "There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running locally have the same system requirements. \n",
    "\n",
    "| Recommended Hardware         | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors| FP32, BF16 |\n",
    "| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n",
    "\n",
    "To execute the reference solution presented here, please use CPU for fine tuning. \n",
    "\n",
    "<a id=\"software-requirements\"></a> \n",
    "## Software Requirements \n",
    "Linux OS (Ubuntu 22.04) is used in this reference solution. Make sure the following dependencies are installed, or if you followed the README file instructions for this notebook run the following cell.\n",
    "\n",
    "1. `sudo apt update`\n",
    "2. `sudo apt install -y build-essential gcc git libgl1-mesa-glx libglib2.0-0 python3-dev`\n",
    "3. Python3.9, Pip/Conda and python3.9-venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec95594-e5bf-4e07-83bd-213f681d0fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass('Enter user password:\\n')\n",
    "command = 'sudo -S apt-get update'\n",
    "os.popen(command, 'w').write(password)\n",
    "command = 'sudo -S apt-get install -y build-essential gcc git libgl1-mesa-glx libglib2.0-0 python3-dev'\n",
    "os.popen(command, 'w').write(password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e233e40-cb7f-4200-997c-ef659f679972",
   "metadata": {},
   "source": [
    "<a id=\"how-it-works\"></a> \n",
    "## How It Works?\n",
    "\n",
    "<a id=\"architecture\"></a> \n",
    "### Architecture\n",
    "![Use_case_flow](assets/e2e_flow_HLS_Disease_Prediction.png)\n",
    "*Figure-1: Architecture of the reference kit* \n",
    "\n",
    "- Uses real-world CESM breast cancer datasets with “multi-modal and multi-model” approaches.\n",
    "- Uses two domain toolkits (Intel® Transfer Learning Toolkit and Intel® Extension for Transformers), Intel® Neural Compressor, other libs/tools, Hugging Face model repo and APIs for [ResNet-50](https://huggingface.co/microsoft/resnet-50) and [ClinicalBert](https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT) models. \n",
    "- The NLP reference Implementation component uses [HF Fine-tuning and Inference Optimization workload](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp), which is optimized for document classification. This NLP workload employs Intel® Neural Compressor and other libraries/tools, and utilizes Hugging Face model repository and APIs for ClinicalBert models. The ClinicalBert model, which is pretrained with a Masked-Language-Modeling task on a large corpus of English language from MIMIC-III data, is fine-tuned with the CESM breast cancer annotation dataset to generate a new BERT model.\n",
    "- The Vision reference Implementation component uses [TLT-based vision workload](https://github.com/IntelAI/transfer-learning), which is optimized for image fine-tuning and inference. This workload utilizes Intel® Transfer Learning Tool and tfhub's ResNet-50 model to fine-tune a new convolutional neural network model with subtracted CESM image dataset. The images are preprocessed by using domain expert-defined segmented regions to reduce redundancies during training.\n",
    "- Predicts diagnosis by using categorized contrast enhanced mammography images and radiologists’ notes separately. Weighted ensemble method is applied to results of sub-models to create the final prediction.\n",
    "\n",
    "<a id=\"get-started\"></a> \n",
    "## Get Started\n",
    "\n",
    "### Download the Reference Kit Repository\n",
    "If previously not done, from terminal create a working directory for the reference kit, clone the [Breast Cancer Prediction Reference Kit](https://github.com/intel/disease-prediction) repository into your working directory and follow the README instructions from the section *Run Using Jupyter Lab* to use the notebook before going any further.\n",
    "```\n",
    "git clone https://github.com/intel/disease-prediction.git brca_multimodal\n",
    "cd brca_multimodal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a697e-db85-4995-8c45-52dd94d9ce46",
   "metadata": {},
   "source": [
    "### DataSet\n",
    "The dataset is a collection of 2,006 high-resolution contrast-enhanced spectral mammography (CESM) images (1003 low energy images and 1003 subtracted CESM images) with annotations of 326 female patients. See Figure-1. Each patient has 8 images, 4 representing each side with two views (Top Down looking and Angled Top View) consisting of low energy and subtracted CESM images. Medical reports, written by radiologists, are provided for each case along with manual segmentation annotation for the abnormal findings in each image. As a preprocessing step, we segment the images based on the manual segmentation to get the region of interest and group annotation notes based on the subject and breast side. \n",
    "\n",
    "  ![CESM Images](assets/cesm_and_annotation.png)\n",
    "\n",
    "*Figure-2: Samples of low energy and subtracted CESM images and Medical reports, written by radiologists from the Categorized contrast enhanced mammography dataset. [(Khaled, 2022)](https://www.nature.com/articles/s41597-022-01238-0)*\n",
    "\n",
    "For more details of the dataset, visit the wikipage of the [CESM](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=109379611#109379611bcab02c187174a288dbcbf95d26179e8) and read [Categorized contrast enhanced mammography dataset for diagnostic and artificial intelligence research](https://www.nature.com/articles/s41597-022-01238-0).\n",
    "\n",
    "#### Setting Up the Data\n",
    "Use the link below to download the image datasets.\n",
    "\n",
    "- [High-resolution Contrast-enhanced spectral mammography (CESM) images](https://faspex.cancerimagingarchive.net/aspera/faspex/external_deliveries/260?passcode=5335d2514638afdaf03237780dcdfec29edf4238#)\n",
    "\n",
    "Copy the downloaded CDD-CESM folder into the *data* directory.\n",
    "\n",
    "**Note:** See this dataset's applicable license for terms and conditions. Intel Corporation does not own the rights to this dataset and does not confer any rights to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bce34-5cf5-4df7-8b66-48dc56b47473",
   "metadata": {},
   "source": [
    "<a id=\"run-using-jupyter-lab\"></a> \n",
    "# Run Using Jupyter Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5c0fc-f0eb-4a16-ba25-c500ecef10c8",
   "metadata": {},
   "source": [
    "### 1. Install Required Workflows and Preprocess the Data\n",
    "\n",
    "> Make sure [High-resolution Contrast-enhanced spectral mammography (CESM) images](https://faspex.cancerimagingarchive.net/aspera/faspex/external_deliveries/260?passcode=5335d2514638afdaf03237780dcdfec29edf4238#) are downloaded and copied into the *data* directory.\n",
    "\n",
    "This step involves the installation of the following components:\n",
    "\n",
    "- HF Fine-tune & Inference Optimization workflow\n",
    "- Transfer Learning based on TLT workflow\n",
    "- Model Zoo for Intel®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58df07b7-8ba9-4ecd-a862-9a3904a2552f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Requirement already satisfied: ipywidgets in ./hls_env/lib/python3.9/site-packages (8.0.6)\n",
      "Requirement already satisfied: jupyterlab in ./hls_env/lib/python3.9/site-packages (4.0.1)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting dataset-librarian\n",
      "  Using cached dataset_librarian-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (8.13.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (6.23.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in ./hls_env/lib/python3.9/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (2.2.0)\n",
      "Requirement already satisfied: tomli in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (2.0.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (3.1.2)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (2.6.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (2.22.1)\n",
      "Requirement already satisfied: packaging in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (23.1)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (0.2.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (6.6.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (6.3.2)\n",
      "Requirement already satisfied: jupyter-core in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (5.3.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab) (2.0.2)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pillow\n",
      "  Using cached Pillow-9.5.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Collecting wget\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn-intelex\n",
      "  Using cached scikit_learn_intelex-2023.1.1-py39-none-manylinux1_x86_64.whl (115 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./hls_env/lib/python3.9/site-packages (from async-lru>=1.0.0->jupyterlab) (4.6.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./hls_env/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyterlab) (3.15.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./hls_env/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: decorator in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./hls_env/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./hls_env/lib/python3.9/site-packages (from jinja2>=3.0.3->jupyterlab) (2.1.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./hls_env/lib/python3.9/site-packages (from jupyter-core->jupyterlab) (3.5.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (3.7.0)\n",
      "Requirement already satisfied: argon2-cffi in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (21.3.0)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.0)\n",
      "Requirement already satisfied: send2trash in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.2)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.6.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.17.1)\n",
      "Requirement already satisfied: overrides in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.5.2)\n",
      "Requirement already satisfied: prometheus-client in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.17.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in ./hls_env/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.4.4)\n",
      "Requirement already satisfied: requests>=2.28 in ./hls_env/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.31.0)\n",
      "Requirement already satisfied: jsonschema>=4.17.3 in ./hls_env/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (4.17.3)\n",
      "Requirement already satisfied: json5>=0.9.0 in ./hls_env/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (0.9.14)\n",
      "Requirement already satisfied: babel>=2.10 in ./hls_env/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab) (2.12.1)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting numpy>=1.20.3\n",
      "  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from pandas->dataset-librarian) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting daal4py==2023.1.1\n",
      "  Using cached daal4py-2023.1.1-py39-none-manylinux1_x86_64.whl (13.9 MB)\n",
      "Collecting scikit-learn>=0.22\n",
      "  Using cached scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Collecting daal==2023.1.1\n",
      "  Using cached daal-2023.1.1-py2.py3-none-manylinux1_x86_64.whl (69.2 MB)\n",
      "Collecting tbb==2021.*\n",
      "  Using cached tbb-2021.9.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied: exceptiongroup in ./hls_env/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./hls_env/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./hls_env/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->jupyterlab) (3.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./hls_env/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (0.19.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./hls_env/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./hls_env/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
      "Requirement already satisfied: rfc3339-validator in ./hls_env/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
      "Requirement already satisfied: pyyaml>=5.3 in ./hls_env/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0)\n",
      "Requirement already satisfied: bleach in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.2.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.8.0)\n",
      "Requirement already satisfied: defusedxml in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.5)\n",
      "Requirement already satisfied: beautifulsoup4 in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.12.2)\n",
      "Requirement already satisfied: tinycss2 in ./hls_env/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in ./hls_env/lib/python3.9/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.17.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./hls_env/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in ./hls_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->dataset-librarian) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./hls_env/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./hls_env/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./hls_env/lib/python3.9/site-packages (from requests>=2.28->jupyterlab-server<3,>=2.19.0->jupyterlab) (3.1.0)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./hls_env/lib/python3.9/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (21.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pure-eval in ./hls_env/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: uri-template in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.2.0)\n",
      "Requirement already satisfied: isoduration in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (20.11.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.13)\n",
      "Requirement already satisfied: fqdn in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./hls_env/lib/python3.9/site-packages (from jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (2.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./hls_env/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./hls_env/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.4.1)\n",
      "Requirement already satisfied: webencodings in ./hls_env/lib/python3.9/site-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: pycparser in ./hls_env/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./hls_env/lib/python3.9/site-packages (from isoduration->jsonschema>=4.17.3->jupyterlab-server<3,>=2.19.0->jupyterlab) (1.2.3)\n",
      "Using legacy 'setup.py install' for docx2txt, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for wget, since package 'wheel' is not installed.\n",
      "Installing collected packages: wget, tbb, pytz, docx2txt, tzdata, tqdm, threadpoolctl, python-dotenv, pillow, numpy, joblib, et-xmlfile, daal, scipy, pandas, openpyxl, daal4py, scikit-learn, scikit-learn-intelex, dataset-librarian\n",
      "  Running setup.py install for wget ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for docx2txt ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed daal-2023.1.1 daal4py-2023.1.1 dataset-librarian-1.0.0 docx2txt-0.8 et-xmlfile-1.1.0 joblib-1.2.0 numpy-1.24.3 openpyxl-3.1.2 pandas-2.0.2 pillow-9.5.0 python-dotenv-1.0.0 pytz-2023.3 scikit-learn-1.2.2 scikit-learn-intelex-2023.1.1 scipy-1.10.1 tbb-2021.9.0 threadpoolctl-3.1.0 tqdm-4.65.0 tzdata-2023.3 wget-3.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting intel-tensorflow==2.11.0\n",
      "  Using cached intel_tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (237.5 MB)\n",
      "Collecting neural-compressor==2.0\n",
      "  Using cached neural_compressor-2.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting numpy==1.22.0\n",
      "  Using cached numpy-1.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Requirement already satisfied: pillow in ./hls_env/lib/python3.9/site-packages (from -r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 4)) (9.5.0)\n",
      "Requirement already satisfied: pyyaml in ./hls_env/lib/python3.9/site-packages (from -r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 5)) (6.0)\n",
      "Collecting tensorflow-hub==0.12.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (23.1)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.15.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (4.6.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: setuptools in ./hls_env/lib/python3.9/site-packages (from intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (58.1.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Collecting prettytable\n",
      "  Using cached prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: scikit-learn in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (1.2.2)\n",
      "Collecting pycocotools\n",
      "  Using cached pycocotools-2.0.6-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: pandas in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.0.2)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (5.9.5)\n",
      "Collecting deprecated\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "Requirement already satisfied: requests in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.31.0)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./hls_env/lib/python3.9/site-packages (from requests->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./hls_env/lib/python3.9/site-packages (from pandas->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./hls_env/lib/python3.9/site-packages (from pandas->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from pandas->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prettytable->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (0.2.6)\n",
      "Collecting matplotlib>=2.1.0\n",
      "  Using cached matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 2)) (3.1.0)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./hls_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (6.6.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./hls_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./hls_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->intel-tensorflow==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/vision_wf/workflows/disease_prediction/requirements.txt (line 1)) (3.15.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, py-cpuinfo, libclang, flatbuffers, wrapt, wheel, werkzeug, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, prettytable, oauthlib, numpy, kiwisolver, keras, importlib-resources, grpcio, google-pasta, gast, fonttools, cycler, contextlib2, cachetools, absl-py, tensorflow-hub, schema, rsa, pyasn1-modules, opt-einsum, opencv-python, markdown, h5py, deprecated, contourpy, astunparse, requests-oauthlib, matplotlib, google-auth, pycocotools, google-auth-oauthlib, tensorboard, neural-compressor, intel-tensorflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 contextlib2-21.6.0 contourpy-1.0.7 cycler-0.11.0 deprecated-1.2.14 flatbuffers-23.5.26 fonttools-4.39.4 gast-0.4.0 google-auth-2.19.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 importlib-resources-5.12.0 intel-tensorflow-2.11.0 keras-2.11.0 kiwisolver-1.4.4 libclang-16.0.0 markdown-3.4.3 matplotlib-3.7.1 neural-compressor-2.0 numpy-1.22.0 oauthlib-3.2.2 opencv-python-4.7.0.72 opt-einsum-3.3.0 prettytable-3.7.0 protobuf-3.19.6 py-cpuinfo-9.0.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycocotools-2.0.6 pyparsing-3.0.9 requests-oauthlib-1.3.1 rsa-4.9 schema-0.7.5 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-estimator-2.11.0 tensorflow-hub-0.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 urllib3-1.26.16 werkzeug-2.3.4 wheel-0.40.0 wrapt-1.15.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting transformers==4.26.0\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting datasets==2.11.0\n",
      "  Using cached datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting neural-compressor==2.1\n",
      "  Using cached neural_compressor-2.1-py3-none-any.whl (1.2 MB)\n",
      "Collecting torch==1.13.1\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp39-cp39-linux_x86_64.whl (199.1 MB)\n",
      "Collecting intel_extension_for_pytorch==1.13.100\n",
      "  Using cached intel_extension_for_pytorch-1.13.100-cp39-cp39-manylinux2014_x86_64.whl (38.0 MB)\n",
      "Collecting intel-extension-for-transformers==1.0.0\n",
      "  Using cached intel_extension_for_transformers-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.8 MB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: requests in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./hls_env/lib/python3.9/site-packages (from transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (23.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Using cached pyarrow-12.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "Requirement already satisfied: pandas in ./hls_env/lib/python3.9/site-packages (from datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2.0.2)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: scikit-learn in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: pycocotools in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (2.0.6)\n",
      "Requirement already satisfied: Pillow in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: deprecated in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.14)\n",
      "Requirement already satisfied: prettytable in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.7.0)\n",
      "Requirement already satisfied: schema in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: py-cpuinfo in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: opencv-python in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (4.7.0.72)\n",
      "Requirement already satisfied: psutil in ./hls_env/lib/python3.9/site-packages (from neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions in ./hls_env/lib/python3.9/site-packages (from torch==1.13.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 5)) (4.6.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./hls_env/lib/python3.9/site-packages (from aiohttp->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./hls_env/lib/python3.9/site-packages (from aiohttp->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (23.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./hls_env/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./hls_env/lib/python3.9/site-packages (from deprecated->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./hls_env/lib/python3.9/site-packages (from pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: wcwidth in ./hls_env/lib/python3.9/site-packages (from prettytable->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.2.6)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in ./hls_env/lib/python3.9/site-packages (from pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in ./hls_env/lib/python3.9/site-packages (from schema->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (21.6.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./hls_env/lib/python3.9/site-packages (from scikit-learn->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (4.39.4)\n",
      "Requirement already satisfied: cycler>=0.10 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./hls_env/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: six>=1.5 in ./hls_env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.11.0->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./hls_env/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools->neural-compressor==2.1->-r /localdisk/aagalleg/demo_bc_test/disease-prediction/hf_nlp/workflows/hf_finetuning_and_inference_nlp/requirements.txt (line 3)) (3.15.0)\n",
      "Installing collected packages: tokenizers, xxhash, torch, regex, pyarrow, multidict, intel_extension_for_pytorch, fsspec, frozenlist, filelock, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, neural-compressor, intel-extension-for-transformers, datasets\n",
      "  Attempting uninstall: neural-compressor\n",
      "    Found existing installation: neural-compressor 2.0\n",
      "    Uninstalling neural-compressor-2.0:\n",
      "      Successfully uninstalled neural-compressor-2.0\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 intel-extension-for-transformers-1.0.0 intel_extension_for_pytorch-1.13.100 multidict-6.0.4 multiprocess-0.70.14 neural-compressor-2.1 pyarrow-12.0.0 regex-2023.5.5 responses-0.18.0 tokenizers-0.13.3 torch-1.13.1+cpu transformers-4.26.0 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/localdisk/aagalleg/demo_bc_test/disease-prediction/hls_env/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!bash setup_workflows.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5b901-c305-44e7-b06f-99267f692935",
   "metadata": {},
   "source": [
    "After the installation process is completed successfully, please execute the cell below to initiate the data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5e9844-59cb-4adc-9028-c7808caab283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please read and accept terms and conditions to be able to to use the dataset API.\n",
      "*********************************************************************************\n",
      "## Purpose of the Model Zoo\n",
      "\n",
      "  - Demonstrate the AI workloads and deep learning models Intel has optimized and validated to run on Intel hardware\n",
      "  - Show how to efficiently execute, train, and deploy Intel-optimized models\n",
      "  - Make it easy to get started running Intel-optimized models on Intel hardware in the cloud or on bare metal\n",
      "\n",
      "***DISCLAIMER: These scripts are not intended for benchmarking Intel platforms.\n",
      "For any performance and/or benchmarking information on specific Intel platforms, visit https://www.intel.ai/blog.***\n",
      "\n",
      "Intel is committed to the respect of human rights and avoiding complicity in human rights abuses, a policy reflected at https://www.intel.com/content/www/us/en/policy/policy-human-rights.html. Accordingly, by accessing the Intel material on this platform you agree that you will not use the material in a product or application that causes or contributes to a violation of an internationally recognized human right.\n",
      "\n",
      "## License\n",
      "The Model Zoo for Intel® Architecture is licensed under https://github.com/IntelAI/models/blob/master/LICENSE.\n",
      "\n",
      "## Datasets\n",
      "To the extent that any public datasets are referenced by Intel or accessed using tools or code on this site those datasets are provided by the third party indicated as the data source. Intel does not create the data, or datasets, and does not warrant their accuracy or quality. By accessing the public dataset(s) you agree to the terms associated with those datasets and that your use complies with the applicable license.\n",
      "\n",
      "Please check the list of datasets used in Model Zoo for Intel® Architecture in https://github.com/IntelAI/models/datasets directory.\n",
      "\n",
      "Intel expressly disclaims the accuracy, adequacy, or completeness of any public datasets, and is not liable for any errors, omissions, or defects in the data, or for any reliance on the data. Intel is not liable for any liability or damages relating to your use of public datasets.\n",
      "\n",
      "*********************************************************************************\n",
      "Do you agree to the terms and conditions? (y/n): "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Starting the data preprocessing -----\n",
      "----- file is saved here : data/annotation/annotation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create folders for classified images\n",
      "data/\n",
      "----- Classifying the images for data preprocessing -----\n",
      "Classify Low energy images\n",
      "Classify Subtracted energy images\n",
      "data/segmented_images/Normal/\n",
      "Creating segmented images for \"Normal\" cases .........\n",
      "Creating segmented images for \"malignant\" cases ..........\n",
      "Creating segmented images for \"Benign\" cases ........\n",
      "Terms and conditions agreed\n",
      "\n",
      "File already exists in data/Medical reports for cases .zip.\n",
      "Please delete it and try again!.\n",
      "\n",
      "\n",
      "File already exists in data/Radiology manual annotations.xlsx.\n",
      "Please delete it and try again!.\n",
      "\n",
      "\n",
      "File already exists in data/Radiology_hand_drawn_segmentations_v2.csv.\n",
      "Please delete it and try again!.\n",
      "\n",
      "Preprocessing has finished.\n"
     ]
    }
   ],
   "source": [
    "#The first time you execute dataset_librarian you will be requested to accept the licensing agreement,\n",
    "#scroll down and accept (y) the agreement to continue. The prosses will end once the \"Preprocessing has finished.\" message appears.\n",
    "import os\n",
    "import pkg_resources\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "package_name = \"dataset_librarian\"\n",
    "package_path = pkg_resources.get_distribution(package_name).location\n",
    "package_path = os.path.join(package_path, package_name)\n",
    "env_file_path = os.path.join(package_path, \".env\")\n",
    "USER_CONSENT = dotenv_values(env_file_path).get(\"USER_CONSENT\")\n",
    "\n",
    "if USER_CONSENT  == \"y\":\n",
    "    !python3.9 -m dataset_librarian.dataset -n brca --download --preprocess -d data/ \n",
    "else:\n",
    "    command = 'python3.9 -m dataset_librarian.dataset -n brca --download --preprocess -d data/; echo \"Preprocessing has finished.\"'\n",
    "    os.popen(command, 'w').write(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447360c8-27d3-4d93-9594-1f3edebc70cd",
   "metadata": {},
   "source": [
    "Once preprocessing has ended, you should have the following files and directories inside the `data/` directory:\n",
    "```\n",
    "data/ \n",
    "    |\n",
    "    └──annotation/\n",
    "        |── annotation.csv\n",
    "        |── testing_data.csv\n",
    "        └── training_data.csv\n",
    "    └──CDD-CESM/\n",
    "        |── Low energy images of CDD-CESM/\n",
    "        └── Subtracted images of CDD-CESM/\n",
    "    └──Medical reports for cases/\n",
    "    └──segmented_images/\n",
    "        |── Benign/\n",
    "        |── Malignant/\n",
    "        └── Normal/\n",
    "    └──train_test_split_images\n",
    "        |── test/\n",
    "        └── train/\n",
    "    └──vision_images\n",
    "        |── Benign/\n",
    "        |── Malignant/\n",
    "        └── Normal/\n",
    "    └──Radiology manual annotations.xlsx\n",
    "    └──Radiology_hand_drawn_segmentations_v2.csv\n",
    "    └──README.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24f317-c892-4db2-99a9-79fa5e9743d5",
   "metadata": {},
   "source": [
    "### 2. Model Building Process\n",
    "\n",
    "To train the multi-model disease prediction, utilize the 'breast_cancer_prediction.py' script along with the arguments outlined in the 'disease_prediction_baremetal.yaml' configuration file, which has the following structure:\n",
    "\n",
    "```\n",
    "disease_prediction_baremetal.yaml\n",
    "    \n",
    "    |\n",
    "    └──overwrite_training_testing_ids\n",
    "    └──output_dir\n",
    "    └──test_size\n",
    "    └──write\n",
    "    └──nlp\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "    └──vision\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "```\n",
    "\n",
    "Descriptions of the previous parameters are as follow:\n",
    "\n",
    "- overwrite_training_testing_ids: uses previously created train and test data sets\n",
    "- output_dir: specifies the location of the output model and inference results\n",
    "- test_size: sets the percentage of the test data set\n",
    "- write: a container parameter that is set to false for bare metal\n",
    "- nlp:\n",
    "  - finetune: runs nlp fine-tuning\n",
    "  - inference: runs nlp inference\n",
    "  - additional parameters for the HF fine-tune and inference optimization workflow (more information available [here](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp/config))\n",
    "\n",
    "- vision:\n",
    "  - finetune: runs vision fine-tuning\n",
    "  - inference: runs vision inference\n",
    "  - additional parameters for the Vision: Transfer Learning Toolkit based on TLT workflow (more information available [here](https://github.com/IntelAI/transfer-learning/tree/f2e83f1614901d44d0fdd66f983de50551691676/workflows/disease_prediction))\n",
    "\n",
    "\n",
    "To solely perform the fine-tuning process, set the '--finetune' flag to 'True' and execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aeb7f6-c6b0-44f2-b674-e4cd753183e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml --finetune True --inference False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d487fe-2931-45d8-8f07-a7cc20175a68",
   "metadata": {},
   "source": [
    "### 3. Running Inference\n",
    "After the models are trained and saved using the script from step 2, load the NLP and vision models using the inference option. This applies a weighted ensemble method to generate a final prediction. To only run inference, set the 'inference' parameter to true, the parameter 'finetune' to false and run the command provided in step 2.\n",
    "\n",
    "> Alternatively, you can combine the training and inference processes into one execution by setting both the 'finetune' and 'inference' parameters to true and running the command provided in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0f93e-615d-48bb-a663-b4ad50371e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml --finetune False --inference True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322d32e-372d-46fd-b2a7-56083969a348",
   "metadata": {},
   "source": [
    "<a id=\"expected-output\"></a> \n",
    "## Expected Output\n",
    "A successful execution of inference returns the confusion matrix of the sub-models and ensembled model, as shown in these example results: \n",
    "```\n",
    "------ Confusion Matrix for Vision model ------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign       18.0     11.000   1.000      0.486\n",
    "Malignant     5.0     32.000   0.000      0.615\n",
    "Normal       14.0      9.000  25.000      0.962\n",
    "Recall        0.6      0.865   0.521      0.652\n",
    "\n",
    "------ Confusion Matrix for NLP model ---------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     25.000      4.000     1.0      0.893\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      0.980\n",
    "Recall      0.833      0.919     1.0      0.930\n",
    "\n",
    "------ Confusion Matrix for Ensemble --------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     26.000      4.000     0.0      0.897\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      1.000\n",
    "Recall      0.867      0.919     1.0      0.939\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40238669-92d9-49db-9fe8-9045700099fe",
   "metadata": {},
   "source": [
    "<a id=\"result-visualization\"></a> \n",
    "## Result Visualization\n",
    "By utilizing the displayed widget, users can access a comprehensive overview that includes radiologists' annotation notes, corresponding subtracted CESM images, and ensemble predictions. Scroll down to see the selected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d43a36-e217-4a0d-8e0c-49a41acd44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import widget_manager\n",
    "importlib.reload(widget_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_env",
   "language": "python",
   "name": "hls_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
