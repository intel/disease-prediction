# uses previously created train and test data sets
overwrite_training_testing_ids: true 
# specifies the location of the output model and inference results
output_dir: "../output/"
# sets the percentage of the test data set
test_size: 0.3
# a container parameter that is set to false for bare metal
write: true

nlp:
  args:
    # Path to pretrained model or model identifier from huggingface.co/models.
    model_name_or_path: "emilyalsentzer/Bio_ClinicalBERT"
    # Pretrained tokenizer name or path if not the same as model_name.
    tokenizer_name: "emilyalsentzer/Bio_ClinicalBERT"
    # Local or Huggingface datasets name.
    dataset: "local" 

    # ==== Required only when dataset: 'local' ====
    local_dataset:
      # Input filename incase of local dataset.
      finetune_input: '../data/annotation/annotation.csv'
      inference_input: '../data/annotation/annotation.csv' 
      # File delimiter.
      delimiter: ","
      features:
        # Label column name.
        class_label: "label"
        # Data column name.
        data_column: "symptoms"
        # Id column name.
        id: "Patient_ID"
      # List of class labels.
      label_list: ["Benign", "Malignant", "Normal"]

          
    # finetune
    finetune: false
    # The implementation of fine-tuning pipeline. Now we support trainer and itrex implementation.
    finetune_impl: "itrex"
    # Data type for finetune pipeline. Support fp32 and bf16 for CPU. Support fp32, tf32, and fp16 for GPU.
    dtype_ft: "fp32"
    # Path of file to write output results.
    finetune_output: "finetune_predictions_report.yaml"

    # inference
    inference: false 
    # The implementation of inference pipeline. Now we support trainer and itrex implementation.
    infer_impl: "itrex"
    # Data type for inference pipeline. Support fp32 and bf16 for CPU. Support fp32, tf32, and fp16 for GPU.
    dtype_inf: "fp32" 
    # Path of file to write output results.
    inference_output: "inference_predictions_report.yaml"

    # The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.
    max_seq_len: 64
    # Whether to execute in sanity check mode.
    smoke_test: false
    # For debugging purposes or quicker training, truncate the number of training examples to this value if set.
    max_train_samples: null
    # For debugging purposes or quicker testing, truncate the number of testing examples to this value if set.
    max_test_samples: null
    # The number of processes to use for the preprocessing.
    preprocessing_num_workers: 32
    # Overwrite the cached training and evaluation sets
    overwrite_cache: true

  training_args:
    # Number of epochs to run.
    num_train_epochs: 8
    # Whether to run training.
    do_train: true
    # Whether to run predictions.
    do_predict: true
    # Batch size per device during training.
    per_device_train_batch_size: 100
    # Batch size per device during evaluation.
    per_device_eval_batch_size: 100
    
vision: 
  args:
    # contains the path for dataset_dir
    dataset_dir: "../data/train_test_split_images"
    # contains the path for segmented dataset_dir
    segmented_dir: "../data/segmented_images"
    # Pretrained model name (default resnetv150)
    model: "resnet_v1_50"
    # Directory where trained model gets saved
    saved_model_dir: "resnet_v1_50/1" 

    # finetune
    # runs vision fine-tuning
    finetune: false
    # Datatype of Finetuning model(default: fp32 , options fp32, bf16)
    dtype_ft: "fp32"
    # Path of file to write output results.
    finetune_output: "finetune_predictions_report.yaml"

    # inference
    # runs vision inference
    inference: false 
    # Datatype of inference (default: fp32 , options fp32, bf16)
    dtype_inf: "fp32" 
    # Path of file to write output results.
    inference_output: "inference_predictions_report.yaml"

  training_args:
    # Batch size for training (default 32)
    batch_size: 32
    # Number of epochs for training
    epochs: 5
    # Enable BF16 by default
    bf16: True

    