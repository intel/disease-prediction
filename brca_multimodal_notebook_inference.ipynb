{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9ea5f1f-3f56-4d50-af00-df54361a5196",
   "metadata": {},
   "source": [
    "# Multi-Modal Disease Prediction\n",
    "Many reference kits  in the bio-medical domain focus on a single-model and single-modal solution. Exclusive reliance on a single method has some limitations, such as impairing the design of robust and accurate classifiers for complex datasets. To overcome these limitations, we provide this multi-modal disease prediction reference kit.\n",
    "\n",
    "Multi-modal disease prediction is an Intel  optimized, end-to-end reference kit for fine-tuning and inference. This reference kit implements a multi-model and multi-modal solution that will help to predict diagnosis by using categorized contrast enhanced mammography data and radiologists’ notes.\n",
    " \n",
    "\n",
    "## **Table of Contents**\n",
    "- [Overview](#overview)\n",
    "- [Validated Hardware Details](#validated-hardware-details)\n",
    "- [Software Requirements](#software-requirements)\n",
    "- [How it Works?](#how-it-works)\n",
    "    - [Architecture](#architecture)\n",
    "    - [Pretraining of the ClinicalBERT Model](#pretraining-of-the-clinicalbert-model)\n",
    "- [Get Started](#get-started)\n",
    "- [Run Using Bare Metal](#run-using-bare-metal) \n",
    "- [Expected Output](#expected-output)\n",
    "- [Summary and Next Steps](#summary-and-next-steps)\n",
    "- [Learn More](#learn-more)\n",
    "- [Support](#support)\n",
    "\n",
    "<a id=\"overview\"></a> \n",
    "## Overview\n",
    "This reference kit demonstrates one possible reference implementation of a multi-model and multi-modal solution. While the vision workflow aims to train an image classifier that takes in contrast-enhanced spectral mammography (CESM) images, the natural language processing (NLP) workflow aims to train a document classifier that takes in annotation notes about a patient’s symptoms. Each pipeline creates prediction for the diagnosis of breast cancer. In the end, weighted ensemble method is used to create final prediction.\n",
    "\n",
    "The goal is to minimize an expert’s involvement in categorizing samples as normal, benign, or malignant, by developing and optimizing a decision support system that automatically categorizes the CESM with the help of radiologist notes.\n",
    "\n",
    "<a id=\"validated-hardware-details\"></a> \n",
    "## Validated Hardware Details\n",
    "There are workflow-specific hardware and software setup requirements depending on how the workflow is run. Bare metal development system and Docker image running locally have the same system requirements. \n",
    "\n",
    "| Recommended Hardware         | Precision  |\n",
    "| ---------------------------- | ---------- |\n",
    "| Intel® 4th Gen Xeon® Scalable Performance processors| FP32, BF16 |\n",
    "| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors| FP32 |\n",
    "\n",
    "To execute the reference solution presented here, please use CPU for fine tuning. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca24f317-c892-4db2-99a9-79fa5e9743d5",
   "metadata": {},
   "source": [
    "### Model Building Process\n",
    "\n",
    "To train the multi-model disease prediction, utilize the 'breast_cancer_prediction.py' script along with the arguments outlined in the 'disease_prediction_baremetal.yaml' configuration file, which has the following structure:\n",
    "\n",
    "```\n",
    "disease_prediction_baremetal.yaml\n",
    "    \n",
    "    |\n",
    "    └──overwrite_training_testing_ids\n",
    "    └──output_dir\n",
    "    └──test_size\n",
    "    └──write\n",
    "    └──nlp\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "    └──vision\n",
    "        |── finetune\n",
    "        |── inference\n",
    "        └── other parameters for HF fine-tune and inference optimization workflow\n",
    "```\n",
    "\n",
    "The 'disease_prediction_baremetal.yaml' file includes the following parameters:\n",
    "\n",
    "- overwrite_training_testing_ids: uses previously created train and test data sets\n",
    "- output_dir: specifies the location of the output model and inference results\n",
    "- test_size: sets the percentage of the test data set\n",
    "- write: a container parameter that is set to false for bare metal\n",
    "- nlp:\n",
    "  - finetune: runs nlp fine-tuning\n",
    "  - inference: runs nlp inference\n",
    "  - additional parameters for the HF fine-tune and inference optimization workflow (more information available [here](https://github.com/intel/intel-extension-for-transformers/tree/main/workflows/hf_finetuning_and_inference_nlp/config))\n",
    "\n",
    "- vision:\n",
    "  - finetune: runs vision fine-tuning\n",
    "  - inference: runs vision inference\n",
    "  - additional parameters for the Vision: Transfer Learning Toolkit based on TLT workflow (more information available [here](https://github.com/IntelAI/transfer-learning/tree/f2e83f1614901d44d0fdd66f983de50551691676/workflows/disease_prediction))\n",
    "\n",
    "\n",
    "To solely perform the fine-tuning process, set the 'finetune' parameter to true in the 'disease_prediction.yaml' file. After the models are trained and saved using the script from step 4, load the NLP and vision models using the inference option. This applies a weighted ensemble method to generate a final prediction. To only run inference, set the 'inference' parameter to true in the 'disease_prediction.yaml' file and and execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aeb7f6-c6b0-44f2-b674-e4cd753183e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training or Testing data does not exist. Creating them.\n",
      "2023-05-22 18:57:07.846240: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Downloading and preparing dataset csv/default to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-e7d7fc2abfb19ce0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 10155.70it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1158.65it/s]\n",
      "Dataset csv downloaded and prepared to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-e7d7fc2abfb19ce0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset csv/default to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-046226449e9e5e6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1844.46it/s]\n",
      "Dataset csv downloaded and prepared to /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-046226449e9e5e6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /nfs/site/home/aagalleg/.cache/huggingface/hub/models--emilyalsentzer--Bio_ClinicalBERT/snapshots/9b5e0380b37eac696b3ff68b5f319c554523971f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2023-05-22 18:57:16 [INFO] ***** Running training *****\n",
      "2023-05-22 18:57:16 [INFO]   Num examples = 380\n",
      "2023-05-22 18:57:16 [INFO]   Num Epochs = 8\n",
      "2023-05-22 18:57:16 [INFO]   Instantaneous batch size per device = 100\n",
      "2023-05-22 18:57:16 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "2023-05-22 18:57:16 [INFO]   Gradient Accumulation steps = 1\n",
      "2023-05-22 18:57:16 [INFO]   Total optimization steps = 32\n",
      "  0%|                                                    | 0/32 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████████████| 32/32 [01:13<00:00,  2.03s/it]2023-05-22 18:58:30 [INFO] \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 73.5147, 'train_samples_per_second': 41.352, 'train_steps_per_second': 0.435, 'train_loss': 0.5703203082084656, 'epoch': 8.0}\n",
      "100%|███████████████████████████████████████████| 32/32 [01:13<00:00,  2.30s/it]\n",
      "2023-05-22 18:58:30 [INFO] Saving model checkpoint to /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp\n",
      "Configuration saved in /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/config.json\n",
      "Model weights saved in /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/pytorch_model.bin\n",
      "tokenizer config file saved in /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/tokenizer_config.json\n",
      "Special tokens file saved in /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/special_tokens_map.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 43\n",
      "  Batch size = 100\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 885.43it/s]\n",
      "\n",
      "*********** TEST_METRICS ***********\n",
      "Accuracy: 0.9302325581395349\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 380\n",
      "  Batch size = 100\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  2.58it/s]\n",
      "2023-05-22 18:58:34.606839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Found cached dataset csv (/nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-046226449e9e5e6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at /nfs/site/home/aagalleg/.cache/huggingface/datasets/csv/default-046226449e9e5e6f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ef6aef8482ff0757.arrow\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/nlp.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 43\n",
      "  Batch size = 100\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1258.04it/s]\n",
      "\n",
      "*********** TEST_METRICS ***********\n",
      "Accuracy: 0.9302325581395349\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 43\n",
      "  Batch size = 100\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1968.23it/s]\n",
      "['/localdisk/aagalleg/dp_demo/disease-prediction/vision_wf/workflows/disease_prediction/src/../../../', '/localdisk/aagalleg/dp_demo/disease-prediction/vision_wf/workflows/disease_prediction/src', '/localdisk/aagalleg/miniconda3/lib/python39.zip', '/localdisk/aagalleg/miniconda3/lib/python3.9', '/localdisk/aagalleg/miniconda3/lib/python3.9/lib-dynload', '/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages']\n",
      "2023-05-22 18:58:43.369753: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "Model Loading time (s):  0.005664348602294922\n",
      "Found 1038 files belonging to 3 classes.\n",
      "2023-05-22 18:58:44.970628: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 18:58:44.974575: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Class names: ['Benign', 'Malignant', 'Normal']\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:From /localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Enabling auto_mixed_precision_mkl\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "Checkpoint directory: /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/vision/resnet_v1_50_checkpoints\n",
      "Epoch 1/5\n",
      "/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-05-22 18:58:49.731195: W tensorflow/core/grappler/optimizers/meta_optimizer.cc:388] NOTE: auto_mixed_precision_mkl is deprecated. Please use auto_mixed_precision_onednn_bfloat16 instead\n",
      "2023-05-22 18:58:49.800235: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 204/1018 nodes to bfloat16 precision using 2 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "     25/Unknown - 18s 569ms/step - loss: 0.8609 - acc: 0.68752023-05-22 18:59:05.737983: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "     26/Unknown - 19s 566ms/step - loss: 0.8653 - acc: 0.56252023-05-22 18:59:06.877265: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 195/809 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "26/26 [==============================] - 24s 797ms/step - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.7840 - val_acc: 0.6771 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "26/26 [==============================] - 20s 788ms/step - loss: 0.5783 - acc: 0.7188 - val_loss: 0.8819 - val_acc: 0.6667 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "26/26 [==============================] - 19s 745ms/step - loss: 0.4649 - acc: 0.7500 - val_loss: 0.8064 - val_acc: 0.6667 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "26/26 [==============================] - 18s 683ms/step - loss: 0.3443 - acc: 0.8125 - val_loss: 0.8816 - val_acc: 0.7135 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "26/26 [==============================] - 17s 640ms/step - loss: 0.2254 - acc: 0.9375 - val_loss: 0.8866 - val_acc: 0.7083 - lr: 0.0010\n",
      "\n",
      "Total Vision Finetuning time (s):  99.65336322784424\n",
      "6/6 [==============================] - 3s 515ms/step - loss: 0.8866 - acc: 0.7083\n",
      "loss: 0.8865634799003601\n",
      "acc: 0.7083333134651184\n",
      "dict_metrics: {'e2e_training_time': 99.65336322784424, 'loss': 0.8865634799003601, 'acc': 0.7083333134651184}\n",
      "Finished Fine-tuning the vision model...\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "Saved model directory: /localdisk/aagalleg/dp_demo/disease-prediction/src/../output/vision/resnet_v1_50/1\n",
      "Done finetuning the vision model ............\n",
      "Found 1038 files belonging to 3 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "\n",
      " Vision Model Loading time:  4.117761135101318\n",
      "Found 1038 files belonging to 3 classes.\n",
      "/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "2023-05-22 19:00:37.299134: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 197/815 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "33/33 [==============================] - 17s 492ms/step - loss: 0.6645 - acc: 0.7351\n",
      "loss: 0.6645209789276123\n",
      "acc: 0.7350674271583557\n",
      "Infering data in folder:  Normal\n",
      "2023-05-22 19:00:54.239281: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2254] Converted 194/752 nodes to bfloat16 precision using 1 cast(s) to bfloat16 (excluding Const and Variable casts)\n",
      "Infering data in folder:  Benign\n",
      "Infering data in folder:  Malignant\n",
      "Vision inference time:  79.15294027328491\n",
      "['/localdisk/aagalleg/dp_demo/disease-prediction/vision_wf/workflows/disease_prediction/src/../../../', '/localdisk/aagalleg/dp_demo/disease-prediction/vision_wf/workflows/disease_prediction/src', '/localdisk/aagalleg/miniconda3/lib/python39.zip', '/localdisk/aagalleg/miniconda3/lib/python3.9', '/localdisk/aagalleg/miniconda3/lib/python3.9/lib-dynload', '/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages']\n",
      "2023-05-22 19:02:14.283323: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Found 1038 files belonging to 3 classes.\n",
      "2023-05-22 19:02:15.830355: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 19:02:15.834373: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23561152  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,185,667\n",
      "Trainable params: 2,624,515\n",
      "Non-trainable params: 23,561,152\n",
      "_________________________________________________________________\n",
      "\n",
      " Vision Model Loading time:  4.077843904495239\n",
      "Found 113 files belonging to 3 classes.\n",
      "/localdisk/aagalleg/dp_demo/disease-prediction/hls_env/lib/python3.9/site-packages/keras/backend.py:5585: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "4/4 [==============================] - 2s 337ms/step - loss: 0.6698 - acc: 0.7345\n",
      "loss: 0.6698063015937805\n",
      "acc: 0.7345132827758789\n",
      "Infering data in folder:  Benign\n",
      "Infering data in folder:  Malignant\n",
      "Infering data in folder:  Normal\n",
      "Vision inference time:  7.926390647888184\n",
      "        Confusion Matrix for vision_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal     13.000     3.0       0.00   0.812\n",
      "Benign      3.000     5.0       4.00   0.417\n",
      "Malignant   1.000     2.0      12.00   0.800\n",
      "Precision   0.765     0.5       0.75   0.698\n",
      "\n",
      "        Confusion Matrix for nlp_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal       16.0   0.000      0.000   1.000\n",
      "Benign        0.0  10.000      2.000   0.833\n",
      "Malignant     0.0   1.000     14.000   0.933\n",
      "Precision     1.0   0.909      0.875   0.930\n",
      "\n",
      "        Confusion Matrix for ensemble_predictions\n",
      "           Normal  Benign  Malignant  Recall\n",
      "Normal       16.0   0.000      0.000   1.000\n",
      "Benign        0.0  10.000      2.000   0.833\n",
      "Malignant     0.0   1.000     14.000   0.933\n",
      "Precision     1.0   0.909      0.875   0.930\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!python src/breast_cancer_prediction.py --config_file configs/disease_prediction_baremetal.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7713485",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "A successful execution of inference returns the confusion matrix of the sub-models and ensembled model, as shown in these example results: \n",
    "```\n",
    "------ Confusion Matrix for Vision model ------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign       18.0     11.000   1.000      0.486\n",
    "Malignant     5.0     32.000   0.000      0.615\n",
    "Normal       14.0      9.000  25.000      0.962\n",
    "Recall        0.6      0.865   0.521      0.652\n",
    "\n",
    "------ Confusion Matrix for NLP model ---------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     25.000      4.000     1.0      0.893\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      0.980\n",
    "Recall      0.833      0.919     1.0      0.930\n",
    "\n",
    "------ Confusion Matrix for Ensemble --------\n",
    "           Benign  Malignant  Normal  Precision\n",
    "Benign     26.000      4.000     0.0      0.897\n",
    "Malignant   3.000     34.000     0.0      0.895\n",
    "Normal      0.000      0.000    48.0      1.000\n",
    "Recall      0.867      0.919     1.0      0.939\n",
    "\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398d6d14",
   "metadata": {},
   "source": [
    "Install dependencies to view interactive results inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e03f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be4f721b35142b694584e89cb1341eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Select(description='Patient ID:', layout=Layout(grid_area='widget001'), options=('Sel…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<module 'widget_manager' from '/localdisk/aagalleg/dp_demo/disease-prediction/widget_manager.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import widget_manager\n",
    "importlib.reload(widget_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
